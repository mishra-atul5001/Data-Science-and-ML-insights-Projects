{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Predicting the Dow Jones with News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Data flow for a Text Related Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/textmining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement & Reference Architecture\n",
    "\n",
    "* **Aim**: Use Reddit News Headlines to predict the movement of Dow Jones Industrial Average.   \n",
    "\n",
    "\n",
    "* **Data Source**: https://www.kaggle.com/aaron7sun/stocknews \n",
    "\n",
    "\n",
    "* **Data Description**: Dow Jones details on Open, High, Low and Close for each day from 2008-08-08 to 2016-07-01 and headlines for those dates from Reddit News. \n",
    "\n",
    "\n",
    "* **Methodology**: For this project, we will use GloVe to create our word embeddings and CNNs followed by LSTMs to build our model. This model is based off the work done in this paper https://www.aclweb.org/anthology/C/C16/C16-1229.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![basic](resources/basic_intent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ign:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\n",
      "Ign:2 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
      "Hit:3 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu xenial InRelease\n",
      "Hit:5 http://security.ubuntu.com/ubuntu xenial-security InRelease\n",
      "Hit:6 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu xenial-updates InRelease        \n",
      "Hit:9 http://archive.ubuntu.com/ubuntu xenial-backports InRelease\n",
      "Reading package lists... Done                     \n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.7.4-0ubuntu1.4).\n",
      "wget is already the newest version (1.17.1-1ubuntu1.4).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 73 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get update  && apt-get install -y --allow-downgrades --no-install-recommends git wget \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "graphviz is already the newest version (2.38.0-12ubuntu2.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 73 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get -y install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from nltk)\n",
      "Requirement already satisfied: keras-applications==1.0.4 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: keras-preprocessing==1.0.2 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.5/dist-packages (from pydot)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.5/dist-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import median_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras Imports\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras.layers import Dropout, Activation, Embedding, Convolution1D, MaxPooling1D, Input, Dense, add, \\\n",
    "                         BatchNormalization, Flatten, Reshape, Concatenate\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras import regularizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dj = pd.read_csv(\"/storage/DowJones.csv\")\n",
    "news = pd.read_csv(\"/storage/News.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>17924.240234</td>\n",
       "      <td>18002.380859</td>\n",
       "      <td>17916.910156</td>\n",
       "      <td>17949.369141</td>\n",
       "      <td>82160000</td>\n",
       "      <td>17949.369141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>17712.759766</td>\n",
       "      <td>17930.609375</td>\n",
       "      <td>17711.800781</td>\n",
       "      <td>17929.990234</td>\n",
       "      <td>133030000</td>\n",
       "      <td>17929.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17704.509766</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17694.679688</td>\n",
       "      <td>106380000</td>\n",
       "      <td>17694.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>112190000</td>\n",
       "      <td>17409.720703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17063.080078</td>\n",
       "      <td>17140.240234</td>\n",
       "      <td>138740000</td>\n",
       "      <td>17140.240234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Open          High           Low         Close  \\\n",
       "0  2016-07-01  17924.240234  18002.380859  17916.910156  17949.369141   \n",
       "1  2016-06-30  17712.759766  17930.609375  17711.800781  17929.990234   \n",
       "2  2016-06-29  17456.019531  17704.509766  17456.019531  17694.679688   \n",
       "3  2016-06-28  17190.509766  17409.720703  17190.509766  17409.720703   \n",
       "4  2016-06-27  17355.210938  17355.210938  17063.080078  17140.240234   \n",
       "\n",
       "      Volume     Adj Close  \n",
       "0   82160000  17949.369141  \n",
       "1  133030000  17929.990234  \n",
       "2  106380000  17694.679688  \n",
       "3  112190000  17409.720703  \n",
       "4  138740000  17140.240234  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Volume       0\n",
       "Adj Close    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.isnull().sum() #No missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date    0\n",
       "News    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.isnull().sum() #No missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               News\n",
       "0  2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
       "1  2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
       "2  2016-07-01  The president of France says if Brexit won, so...\n",
       "3  2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
       "4  2016-07-01  100+ Nobel laureates urge Greenpeace to stop o..."
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1989, 7)\n",
      "(73608, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dj.shape)\n",
    "print(news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989\n",
      "2943\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of unique dates. We want matching values.\n",
    "print(len(set(dj.Date)))\n",
    "print(len(set(news.Date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the extra dates that are in news\n",
    "news = news[news.Date.isin(dj.Date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989\n",
      "1989\n"
     ]
    }
   ],
   "source": [
    "print(len(set(dj.Date)))\n",
    "print(len(set(news.Date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>17924.240234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>17712.759766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>17456.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>17190.509766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>17355.210938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Open\n",
       "0  2016-07-01  17924.240234\n",
       "1  2016-06-30  17712.759766\n",
       "2  2016-06-29  17456.019531\n",
       "3  2016-06-28  17190.509766\n",
       "4  2016-06-27  17355.210938"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unwanted features - keep the 'Open' price only\n",
    "dj = dj.drop(['High','Low','Close','Volume','Adj Close'], 1)\n",
    "dj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-07-01</th>\n",
       "      <td>17924.240234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>17712.759766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-29</th>\n",
       "      <td>17456.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-28</th>\n",
       "      <td>17190.509766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-27</th>\n",
       "      <td>17355.210938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open\n",
       "Date                    \n",
       "2016-07-01  17924.240234\n",
       "2016-06-30  17712.759766\n",
       "2016-06-29  17456.019531\n",
       "2016-06-28  17190.509766\n",
       "2016-06-27  17355.210938"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the difference in opening prices between the following and current day.\n",
    "# The model will try to predict the change in Open value based on the today's news.\n",
    "dj = dj.set_index('Date')\n",
    "dj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Target variable = Tomorrow's Open Price - Today's Open Price\n",
    "dj = -1 * dj.diff(periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-07-01</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>211.480468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-29</th>\n",
       "      <td>256.740235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-28</th>\n",
       "      <td>265.509765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-27</th>\n",
       "      <td>-164.701172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open\n",
       "Date                  \n",
       "2016-07-01         NaN\n",
       "2016-06-30  211.480468\n",
       "2016-06-29  256.740235\n",
       "2016-06-28  265.509765\n",
       "2016-06-27 -164.701172"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dj['Date'] = dj.index\n",
    "dj = dj.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211.480468</td>\n",
       "      <td>2016-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256.740235</td>\n",
       "      <td>2016-06-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>265.509765</td>\n",
       "      <td>2016-06-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-164.701172</td>\n",
       "      <td>2016-06-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open        Date\n",
       "0         NaN  2016-07-01\n",
       "1  211.480468  2016-06-30\n",
       "2  256.740235  2016-06-29\n",
       "3  265.509765  2016-06-28\n",
       "4 -164.701172  2016-06-27"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove top row since it has a null value.\n",
    "dj = dj[dj.Open.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open    0\n",
       "Date    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are any more null values.\n",
    "dj.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the two datasets - For each date, get all the headlines and the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the opening prices and their corresponding daily headlines from the news\n",
    "# Define/Initialize the variables\n",
    "price = []\n",
    "headlines = []\n",
    "\n",
    "# For all the rows in the dataframe\n",
    "for row in dj.iterrows():\n",
    "    # define a new variable to store all the headlines for the day\n",
    "    daily_headlines = []\n",
    "    # Spot the date in the given row\n",
    "    date = row[1]['Date']\n",
    "    # Store the price for the date\n",
    "    price.append(row[1]['Open'])\n",
    "    for row_ in news[news.Date==date].iterrows():\n",
    "        daily_headlines.append(row_[1]['News'])\n",
    "\n",
    "    # Append the headlines for the date\n",
    "    headlines.append(daily_headlines)\n",
    "    # Track progress\n",
    "    if len(price) % 500 == 0:\n",
    "        print(len(price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table size=\"100\">\n",
    "    <tr>\n",
    "        <td>headlines</td>\n",
    "        <td>price</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>headline-1, headline-2 ..., headline-n</td>\n",
    "        <td>211.48</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Jamaica proposes marijuana dispensers for tourists at airports following legalisation: The kiosks and desks would give people a license to purchase up to 2 ounces of the drug to use during their stay',\n",
       "   \"Stephen Hawking says pollution and 'stupidity' still biggest threats to mankind: we have certainly not become less greedy or less stupid in our treatment of the environment over the past decade\",\n",
       "   'Boris Johnson says he will not run for Tory party leadership',\n",
       "   'Six gay men in Ivory Coast were abused and forced to flee their homes after they were pictured signing a condolence book for victims of the recent attack on a gay nightclub in Florida',\n",
       "   'Switzerland denies citizenship to Muslim immigrant girls who refused to swim with boys: report',\n",
       "   'Palestinian terrorist stabs israeli teen girl to death in her bedroom',\n",
       "   'Puerto Rico will default on $1 billion of debt on Friday',\n",
       "   'Republic of Ireland fans to be awarded medal for sportsmanship by Paris mayor.',\n",
       "   \"Afghan suicide bomber 'kills up to 40' - BBC News\",\n",
       "   'US airstrikes kill at least 250 ISIS fighters in convoy outside Fallujah, official says',\n",
       "   'Turkish Cop Who Took Down Istanbul Gunman Hailed a Hero',\n",
       "   \"Cannabis compounds could treat Alzheimer's by removing plaque-forming proteins from brain cells, research suggests\",\n",
       "   \"Japan's top court has approved blanket surveillance of the country's Muslims: 'They made us terrorist suspects, we never did anything wrong,' says Japanese Muslim, Mohammed Fujita\",\n",
       "   'CIA Gave Romania Millions to Host Secret Prisons',\n",
       "   'Groups urge U.N. to suspend Saudi Arabia from rights council',\n",
       "   'Googles free wifi at Indian railway stations is better than most of the countrys paid services',\n",
       "   \"Mounting evidence suggests 'hobbits' were wiped out by modern humans' ancestors 50,000 years ago.\",\n",
       "   \"The men who carried out Tuesday's terror attack at Istanbul's Ataturk Airport were from Russia, Uzbekistan and Kyrgyzstan, a Turkish offical said.\",\n",
       "   'Calls to suspend Saudi Arabia from UN Human Rights Council because of military aggresion in Yemen',\n",
       "   'More Than 100 Nobel Laureates Call Out Greenpeace For Anti-GMO Obstruction In Developing World',\n",
       "   'British pedophile sentenced to 85 years in US for trafficking child abuse images: Domminich Shaw, a kingpin of sexual violence against children, sent dozens of images online and discussed plans to assault and kill a child while on probation',\n",
       "   'US permitted 1,200 offshore fracks in Gulf of Mexico between 2010 and 2014 and allowed 72 billion gallons of chemical discharge in 2014.',\n",
       "   'We will be swimming in ridicule - French beach police to carry guns while in swimming trunks: Police lifeguards on Frances busiest beaches will carry guns and bullet-proof vests for the first time this summer amid fears that terrorists could target holidaymakers.',\n",
       "   \"UEFA says no minutes of silence for Istanbul victims at Euro 2016 because 'Turkey have already been eliminated'\",\n",
       "   'Law Enforcement Sources: Gun Used in Paris Terrorist Attacks Came from Phoenix']],\n",
       " [211.48046800000157])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how headlines look like\n",
    "headlines[:1], price[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the price list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[211.48046800000157, 256.7402349999975]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize opening prices (target values)\n",
    "max_price = max(price)\n",
    "min_price = min(price)\n",
    "mean_price = np.mean(price)\n",
    "def normalize(price):\n",
    "    return ((price-min_price)/(max_price-min_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_price = []\n",
    "for p in price:\n",
    "    norm_price.append(normalize(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.4551577545098642\n"
     ]
    }
   ],
   "source": [
    "# Check that normalization worked well\n",
    "print(min(norm_price))\n",
    "print(max(norm_price))\n",
    "print(np.mean(norm_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "22\n",
      "24.996478873239436\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of headlines for each day\n",
    "print(max(len(i) for i in headlines))\n",
    "print(min(len(i) for i in headlines))\n",
    "print(np.mean([len(i) for i in headlines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5780280759194737, 0.6047364662478155]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_price[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the headlines list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I should have gone to dentist so my teeth would not hurt\n",
      "But I am good now\n"
     ]
    }
   ],
   "source": [
    "# remove contractions\n",
    "def decontracted(phrase):\n",
    "    if \"'\" in phrase:\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "text = \"I should've gone to dentist so my teeth wouldn't hurt\"\n",
    "text1 = \"But I am good now\"\n",
    "print(decontracted(text))\n",
    "print(decontracted(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Remove unwanted characters and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        # Remove the contractions\n",
    "        for word in text:\n",
    "            new_text.append(decontracted(word))\n",
    "        # Recreate the sentence\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'0,0', '00', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\\$', ' $ ', text)\n",
    "    text = re.sub(r'u s ', ' united states ', text)\n",
    "    text = re.sub(r'u n ', ' united nations ', text)\n",
    "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
    "    text = re.sub(r'j k ', ' jk ', text)\n",
    "    text = re.sub(r' s ', ' ', text)\n",
    "    text = re.sub(r' yr ', ' year ', text)\n",
    "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
    "    text = re.sub(r'0km ', '0 km ', text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the headlines\n",
    "clean_headlines = []\n",
    "\n",
    "for daily_headlines in headlines:\n",
    "    clean_daily_headlines = []\n",
    "    for headline in daily_headlines:\n",
    "        clean_daily_headlines.append(clean_text(headline))\n",
    "    clean_headlines.append(clean_daily_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jamaica proposes marijuana dispensers tourists airports following legalisation kiosks desks would give people license purchase 2 ounces drug use stay',\n",
       "  'stephen hawking says pollution istupidity still biggest threats mankind certainly become less greedy less stupid treatment environment past decade',\n",
       "  'boris johnson says run tory party leadership',\n",
       "  'six gay men ivory coast abused forced flee homes pictured signing condolence book victims recent attack gay nightclub florida',\n",
       "  'switzerland denies citizenship muslim immigrant girls refused swim boys report',\n",
       "  'palestinian terrorist stabs israeli teen girl death bedroom',\n",
       "  'puerto rico default $ 1 billion debt friday',\n",
       "  'republic ireland fans awarded medal sportsmanship paris mayor',\n",
       "  'afghan suicide bomber kills 40 bbc news',\n",
       "  'us airstrikes kill least 250 isis fighters convoy outside fallujah official says',\n",
       "  'turkish cop took istanbul gunman hailed hero',\n",
       "  'cannabis compounds could treat alzheimer removing plaque forming proteins brain cells research suggests',\n",
       "  'japan top court approved blanket surveillance country muslims nothey made us terrorist suspects never anything wrong says japanese muslim mohammed fujita',\n",
       "  'cia gave romania millions host secret prisons',\n",
       "  'groups urge united nations suspend saudi arabia rights council',\n",
       "  'googles free wifi indian railway stations better countrys paid services',\n",
       "  'mounting evidence suggests hobbits wiped modern humans ancestors 50000 years ago',\n",
       "  'men carried tuesday terror attack istanbul ataturk airport russia uzbekistan kyrgyzstan turkish offical said',\n",
       "  'calls suspend saudi arabia un human rights council military aggresion yemen',\n",
       "  '100 nobel laureates call greenpeace anti gmo obstruction developing world',\n",
       "  'british pedophile sentenced 85 years us trafficking child abuse images domminich shaw kingpin sexual violence children sent dozens images online discussed plans assault kill child probation',\n",
       "  'us permitted 1 200 offshore fracks gulf mexico 2010 2014 allowed 72 billion gallons chemical discharge 2014',\n",
       "  'swimming ridicule french beach police carry guns swimming trunks police lifeguards frances busiest beaches carry guns bullet proof vests first time summer amid fears terrorists could target holidaymakers',\n",
       "  'uefa says minutes silence istanbul victims euro 2016 noturkey already eliminated',\n",
       "  'law enforcement sources gun used paris terrorist attacks came phoenix'],\n",
       " ['explosion airport istanbul',\n",
       "  'yemeni former president terrorism offspring wahhabism al saud regime',\n",
       "  'uk must accept freedom movement access eu market',\n",
       "  'devastated scientists late captive breed mammal lost climate change australian conservationists spent 5 months obtaining permissions planning captive breeding program arrived rodents tiny island late',\n",
       "  'british labor party leader jeremy corbyn loses confidence vote refuses resign',\n",
       "  'muslim shop uk firebombed people inside',\n",
       "  'mexican authorities sexually torture women prison',\n",
       "  'uk shares pound continue recover',\n",
       "  'iceland historian johannesson wins presidential election',\n",
       "  '99 million year old bird wings found encased amber finding things trapped amber far rare researchers burma found pair tiny bird like wings frozen inside knew something special',\n",
       "  'chatbot programmed british teenager successfully challenged 160000 parking tickets since launch last year',\n",
       "  'philippine president elect said monday would aggressively promote artificial birth control country even risk getting fight dominant catholic church staunchly opposes use contraceptives',\n",
       "  'former belgian prime minister ridicules nigel farage accuses ukip leader lying eu referendum campaign',\n",
       "  'brexiteer nigel farage eu laughing',\n",
       "  'islamic state bombings southern yemen kill 38 people',\n",
       "  'escape tunnel dug hand found holocaust massacre site',\n",
       "  'land beijing sinking much four inches per year overconsumption groundwater according new research',\n",
       "  'car bomb anti islamic attack mosque perth australia',\n",
       "  'emaciated lions taiz zoo trapped blood soaked cages left starve months due yemeni civil war',\n",
       "  'rupert murdoch describes brexit wonderful media mogul likened leaving eu prison break shared view donald trump able man',\n",
       "  '40 killed yemen suicide attacks',\n",
       "  'google found disastrous symantec norton vulnerabilities bad gets',\n",
       "  'extremist violence rise germany domestic intelligence agency says far right far left islamist radical groups gaining membership country',\n",
       "  'bbc news labour mps pass corbyn confidence motion',\n",
       "  'tiny new zealand town notoo many jobs launches drive recruit outsiders']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at some headlines to ensure everything was cleaned well\n",
    "clean_headlines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roughly the number of unique words in English: 36311\n"
     ]
    }
   ],
   "source": [
    "print('Roughly the number of unique words in English: {}'.format(len({word: None \n",
    "                                                                      for headlines in clean_headlines \n",
    "                                                                      for headline in headlines \n",
    "                                                                      for word in headline.split()})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the word vocab\n",
    "import collections\n",
    "words = [word for headlines in clean_headlines for headline in headlines for word in headline.split()]\n",
    "word_counts = collections.Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'encampment': 2,\n",
       "         'stop': 443,\n",
       "         'discount': 3,\n",
       "         'messages': 56,\n",
       "         'humane': 8,\n",
       "         'djibouti': 3,\n",
       "         'donors': 14,\n",
       "         'offsets': 1,\n",
       "         'sane': 4,\n",
       "         'supporting': 51,\n",
       "         'spectacle': 3,\n",
       "         'fingerprinting': 3,\n",
       "         'weaponized': 2,\n",
       "         'tallies': 1,\n",
       "         'lourdes': 1,\n",
       "         'marshall': 9,\n",
       "         'stille': 1,\n",
       "         'permission': 44,\n",
       "         'umayyad': 1,\n",
       "         'multiplied': 2,\n",
       "         'float': 6,\n",
       "         'propulsion': 1,\n",
       "         'drawbridge': 1,\n",
       "         'obey': 10,\n",
       "         'obliterated': 2,\n",
       "         '220': 12,\n",
       "         'purchases': 11,\n",
       "         '\\\\r\\\\nleast': 1,\n",
       "         'iscores': 6,\n",
       "         'isuperstate': 1,\n",
       "         'elements': 10,\n",
       "         'iohannis': 1,\n",
       "         'bala': 2,\n",
       "         'educator': 1,\n",
       "         'mathematical': 3,\n",
       "         'dispensing': 1,\n",
       "         'masterminded': 5,\n",
       "         'pinned': 5,\n",
       "         'reagan': 7,\n",
       "         'originates': 2,\n",
       "         'code': 40,\n",
       "         'elementary': 7,\n",
       "         'wouldeep': 6,\n",
       "         'westfield': 1,\n",
       "         'booked': 2,\n",
       "         'centrifugal': 1,\n",
       "         'variety': 5,\n",
       "         'pinter': 1,\n",
       "         'anybody': 16,\n",
       "         'jp': 10,\n",
       "         'blackmailed': 4,\n",
       "         'policymakers': 1,\n",
       "         'infra': 2,\n",
       "         'explains': 30,\n",
       "         'prosper': 1,\n",
       "         'papademos': 1,\n",
       "         'pantomime': 1,\n",
       "         'unidentified': 18,\n",
       "         'sneaked': 1,\n",
       "         'edvige': 1,\n",
       "         'torment': 2,\n",
       "         'finished': 25,\n",
       "         'many': 450,\n",
       "         'amovie': 2,\n",
       "         'unfreeze': 3,\n",
       "         'carcass': 4,\n",
       "         '799': 1,\n",
       "         'consul': 3,\n",
       "         'destined': 8,\n",
       "         'micrograms': 2,\n",
       "         'tsa': 12,\n",
       "         'banning': 74,\n",
       "         'dictatorships': 2,\n",
       "         'peterhead': 1,\n",
       "         'panamanian': 2,\n",
       "         'vomited': 1,\n",
       "         'jr': 2,\n",
       "         'detonators': 1,\n",
       "         'interviewing': 2,\n",
       "         'demonstrations': 65,\n",
       "         'vendors': 5,\n",
       "         'africans': 25,\n",
       "         'layoffs': 5,\n",
       "         'tomislav': 1,\n",
       "         'peopleunlike': 1,\n",
       "         'solidarity': 36,\n",
       "         'reject': 45,\n",
       "         'alaak': 1,\n",
       "         'radar': 21,\n",
       "         'zinedine': 2,\n",
       "         'indictments': 4,\n",
       "         'jeddahs': 1,\n",
       "         'marked': 21,\n",
       "         'eastward': 1,\n",
       "         'correlated': 1,\n",
       "         'von': 2,\n",
       "         'cruely': 1,\n",
       "         'vp': 9,\n",
       "         'spells': 4,\n",
       "         'benefit': 28,\n",
       "         'marches': 17,\n",
       "         'hop': 6,\n",
       "         'whalers': 14,\n",
       "         'defect': 16,\n",
       "         'bracelets': 2,\n",
       "         '43': 61,\n",
       "         'cadburys': 1,\n",
       "         'copyfraud': 1,\n",
       "         'householders': 1,\n",
       "         'grid': 24,\n",
       "         'footage': 123,\n",
       "         'nothefts': 1,\n",
       "         'appreciated': 3,\n",
       "         'maggie': 1,\n",
       "         'zuist': 1,\n",
       "         'flit': 1,\n",
       "         'meshing': 1,\n",
       "         'weichel': 1,\n",
       "         'kiribati': 9,\n",
       "         'financier': 1,\n",
       "         'defiance': 16,\n",
       "         'isexualising': 1,\n",
       "         'undocumented': 7,\n",
       "         'image': 53,\n",
       "         'motherland': 2,\n",
       "         'achievement': 3,\n",
       "         'healer': 4,\n",
       "         'mannings': 2,\n",
       "         'unmask': 2,\n",
       "         'mccarthy': 1,\n",
       "         '530k': 1,\n",
       "         'undo': 4,\n",
       "         'youngsters': 3,\n",
       "         '20c': 1,\n",
       "         'finns': 7,\n",
       "         'warships': 51,\n",
       "         'wiping': 16,\n",
       "         'forehead': 2,\n",
       "         'devastated': 14,\n",
       "         'irregulars': 1,\n",
       "         'skeletal': 1,\n",
       "         '269': 1,\n",
       "         'echoing': 2,\n",
       "         'revoking': 6,\n",
       "         'mourn': 3,\n",
       "         'jab': 5,\n",
       "         'amicrowave': 2,\n",
       "         'conflict': 200,\n",
       "         'evolving': 3,\n",
       "         'handkerchiefs': 1,\n",
       "         'shatters': 4,\n",
       "         'unplug': 1,\n",
       "         'baikal': 1,\n",
       "         'nukes': 30,\n",
       "         'todenhfer': 1,\n",
       "         'relocated': 6,\n",
       "         'spilling': 11,\n",
       "         'fails': 70,\n",
       "         'radisson': 2,\n",
       "         'overshoot': 1,\n",
       "         'r/nature': 1,\n",
       "         'we\\\\': 21,\n",
       "         'mating': 3,\n",
       "         '211': 2,\n",
       "         'daubs': 1,\n",
       "         'majedah': 1,\n",
       "         'reich': 2,\n",
       "         'commercialize': 3,\n",
       "         'tokio': 1,\n",
       "         'imperial': 9,\n",
       "         'tech': 56,\n",
       "         'inaccurate': 3,\n",
       "         'witchs': 1,\n",
       "         'discovers': 25,\n",
       "         'mistreated': 1,\n",
       "         'hughes': 2,\n",
       "         'provinces': 6,\n",
       "         'oregon': 8,\n",
       "         '06': 1,\n",
       "         'gordievsky': 1,\n",
       "         'hour': 108,\n",
       "         'zebras': 3,\n",
       "         'assaulting': 10,\n",
       "         'pedestrianise': 1,\n",
       "         'tactful': 1,\n",
       "         'rates': 81,\n",
       "         'missions': 22,\n",
       "         'wouldemonisation': 1,\n",
       "         'neuro': 1,\n",
       "         'far': 276,\n",
       "         'episode': 9,\n",
       "         'rupee': 2,\n",
       "         'unaffected': 2,\n",
       "         'semitism': 24,\n",
       "         'proxies': 3,\n",
       "         'onto': 38,\n",
       "         'prestige': 3,\n",
       "         'demolishes': 12,\n",
       "         'isettlers': 6,\n",
       "         'cradle': 2,\n",
       "         'notion': 7,\n",
       "         'secretary': 120,\n",
       "         'visable': 1,\n",
       "         'chafe': 1,\n",
       "         'depreciating': 1,\n",
       "         'decoded': 1,\n",
       "         'alitalia': 1,\n",
       "         'constructing': 6,\n",
       "         'antarctic': 46,\n",
       "         'chunk': 8,\n",
       "         'hancock': 2,\n",
       "         'lawsuit': 76,\n",
       "         'wheel': 8,\n",
       "         'rays': 9,\n",
       "         'pangolins': 3,\n",
       "         'koy4goff': 1,\n",
       "         'ukrainians': 10,\n",
       "         'snowden': 445,\n",
       "         'quasicrystals': 1,\n",
       "         'offers': 100,\n",
       "         'truthdig': 3,\n",
       "         'corpse': 14,\n",
       "         'notice': 29,\n",
       "         'headteacher': 2,\n",
       "         'desires': 2,\n",
       "         'pirates': 131,\n",
       "         'disrupting': 11,\n",
       "         'mcgarahan': 1,\n",
       "         'kumar': 3,\n",
       "         'mccarthyism': 4,\n",
       "         'zardari': 8,\n",
       "         'dagan': 1,\n",
       "         'present': 39,\n",
       "         'internet': 688,\n",
       "         'cruisers': 2,\n",
       "         'recalls': 14,\n",
       "         'rbs': 14,\n",
       "         'sects': 2,\n",
       "         'dieselgate': 2,\n",
       "         'wiretap': 9,\n",
       "         'stool': 1,\n",
       "         'alqsa': 1,\n",
       "         'vasectomies': 1,\n",
       "         'wooed': 1,\n",
       "         'ulterior': 1,\n",
       "         'remodeled': 1,\n",
       "         'shark': 59,\n",
       "         'apologized': 1,\n",
       "         'isteer': 1,\n",
       "         '1million': 3,\n",
       "         'nunavut': 2,\n",
       "         'priced': 1,\n",
       "         'daewoo': 1,\n",
       "         'borghezio': 1,\n",
       "         'infringed': 2,\n",
       "         'scandalous': 1,\n",
       "         'tinfoil': 1,\n",
       "         'dubai\\\\': 1,\n",
       "         'guttatus': 1,\n",
       "         'tyrannosaurus': 1,\n",
       "         'mania': 3,\n",
       "         'hospitals': 50,\n",
       "         'sabahi': 1,\n",
       "         'involved': 111,\n",
       "         'tundra': 3,\n",
       "         'drew': 13,\n",
       "         'towels': 1,\n",
       "         'emergency': 170,\n",
       "         'accuses': 120,\n",
       "         'fur': 5,\n",
       "         'eyed': 7,\n",
       "         'mediterranean': 36,\n",
       "         'amumbai': 13,\n",
       "         'paternity': 3,\n",
       "         'committing': 24,\n",
       "         'gamed': 1,\n",
       "         'enlightened': 1,\n",
       "         'overweight': 10,\n",
       "         '500000': 40,\n",
       "         'flown': 20,\n",
       "         'basics': 2,\n",
       "         '1940': 1,\n",
       "         'elevated': 5,\n",
       "         'kadhafi': 2,\n",
       "         'qalamoun': 3,\n",
       "         'markers': 1,\n",
       "         'allahu': 3,\n",
       "         'tells': 235,\n",
       "         '88th': 2,\n",
       "         'd\\\\': 1,\n",
       "         '177': 2,\n",
       "         'amulets': 1,\n",
       "         'worshipping': 2,\n",
       "         'hari': 6,\n",
       "         'sighting': 2,\n",
       "         'nazis\\\\': 1,\n",
       "         'move': 226,\n",
       "         'indebted': 5,\n",
       "         'warmest': 10,\n",
       "         'rearranging': 1,\n",
       "         'strive': 1,\n",
       "         'designing': 2,\n",
       "         'reporters': 60,\n",
       "         'modernize': 2,\n",
       "         'dmitry': 23,\n",
       "         'swiftly': 3,\n",
       "         'overtaken': 8,\n",
       "         'miniskirts': 3,\n",
       "         'tattooed': 4,\n",
       "         'regulates': 3,\n",
       "         '089': 1,\n",
       "         'marketting': 1,\n",
       "         'joyce': 1,\n",
       "         'burdened': 1,\n",
       "         'striker': 1,\n",
       "         'shrugs': 2,\n",
       "         'editor': 53,\n",
       "         'tsunami': 119,\n",
       "         'bangledeshi': 1,\n",
       "         'anchoring': 1,\n",
       "         'outcomes': 2,\n",
       "         'allied': 15,\n",
       "         'amohammed': 1,\n",
       "         'pct': 2,\n",
       "         'mansions': 5,\n",
       "         'dogged': 1,\n",
       "         'souief': 1,\n",
       "         'concrete': 27,\n",
       "         'distributions': 1,\n",
       "         'sturman': 1,\n",
       "         'gujjar': 1,\n",
       "         'ejection': 1,\n",
       "         'lobsters': 3,\n",
       "         'chapters': 2,\n",
       "         'yeates': 1,\n",
       "         'songs': 7,\n",
       "         'obviousness': 1,\n",
       "         '200kg': 1,\n",
       "         'southwest': 10,\n",
       "         '65': 44,\n",
       "         'soiled': 1,\n",
       "         'hotspots': 5,\n",
       "         'quartz': 2,\n",
       "         'rulers': 29,\n",
       "         'incentive': 4,\n",
       "         'discounts': 1,\n",
       "         'generates': 5,\n",
       "         'daughter': 137,\n",
       "         'reconstruction': 21,\n",
       "         'lyon': 2,\n",
       "         '16th': 3,\n",
       "         'opera': 7,\n",
       "         'destinations': 4,\n",
       "         'rested': 1,\n",
       "         'relentlessly': 4,\n",
       "         'tempted': 4,\n",
       "         'harpers': 4,\n",
       "         'bioenergy': 1,\n",
       "         'speakers': 13,\n",
       "         'washpost': 1,\n",
       "         'caved': 3,\n",
       "         '5': 575,\n",
       "         'uglier': 1,\n",
       "         'headache': 6,\n",
       "         'wagering': 1,\n",
       "         '000sq': 1,\n",
       "         'safety': 85,\n",
       "         'mercenaries': 16,\n",
       "         'argeement': 1,\n",
       "         'rush': 21,\n",
       "         'clamps': 3,\n",
       "         '361': 1,\n",
       "         'precautionary': 3,\n",
       "         'killed/wounded': 1,\n",
       "         'franks': 1,\n",
       "         'livestock': 9,\n",
       "         'kosovars': 1,\n",
       "         'mementos': 2,\n",
       "         'ego': 2,\n",
       "         'adm': 1,\n",
       "         'referees': 1,\n",
       "         'isupport': 3,\n",
       "         'istrategic': 1,\n",
       "         'idriss': 1,\n",
       "         'divesting': 2,\n",
       "         'emailing': 1,\n",
       "         'gallery': 12,\n",
       "         'unconvincing': 1,\n",
       "         'perjury': 2,\n",
       "         'yorkshire': 5,\n",
       "         'goodies': 1,\n",
       "         'gaddafis': 4,\n",
       "         'quotas': 11,\n",
       "         '172': 1,\n",
       "         'drenched': 3,\n",
       "         'shunning': 2,\n",
       "         'learned': 23,\n",
       "         'commie': 1,\n",
       "         'upstate': 1,\n",
       "         'jihadis': 21,\n",
       "         'iseizes': 2,\n",
       "         'hillsong': 1,\n",
       "         'energy': 350,\n",
       "         '\\\\nmegrahi': 1,\n",
       "         'congressional': 7,\n",
       "         'hijacking': 8,\n",
       "         'verona': 1,\n",
       "         'deforester': 1,\n",
       "         'offending': 15,\n",
       "         'relative': 12,\n",
       "         'fiber': 6,\n",
       "         'sickness': 7,\n",
       "         'skydive': 2,\n",
       "         'committee': 82,\n",
       "         'swifter': 1,\n",
       "         'contravention': 1,\n",
       "         'mammals': 11,\n",
       "         'noriega': 1,\n",
       "         'concepts': 3,\n",
       "         'elbashayer': 1,\n",
       "         'yakuza': 4,\n",
       "         'ammo': 5,\n",
       "         'incidentally': 1,\n",
       "         'sochi': 40,\n",
       "         'prince\\\\': 1,\n",
       "         'ps4': 1,\n",
       "         'finances': 12,\n",
       "         'mustansir': 1,\n",
       "         'funny': 9,\n",
       "         'exchanged': 5,\n",
       "         'bracelet': 2,\n",
       "         'cherry': 3,\n",
       "         'legalization': 69,\n",
       "         'mcnamara': 1,\n",
       "         'nephew': 6,\n",
       "         'crowning': 1,\n",
       "         'pundits': 2,\n",
       "         'circulating': 4,\n",
       "         'bite': 10,\n",
       "         'islam4uk': 2,\n",
       "         'gusher': 1,\n",
       "         'posh': 4,\n",
       "         'authorising': 1,\n",
       "         'exempt': 6,\n",
       "         'blackface': 1,\n",
       "         'condemn': 39,\n",
       "         'unionists': 2,\n",
       "         'ishell': 4,\n",
       "         'bardot\\\\': 1,\n",
       "         'defiant': 20,\n",
       "         'spins': 2,\n",
       "         'waiving': 1,\n",
       "         'hurried': 2,\n",
       "         'crickhowell': 1,\n",
       "         'chens': 1,\n",
       "         'doctrinal': 1,\n",
       "         'heartland': 4,\n",
       "         'supervoid': 1,\n",
       "         'counterfeits': 3,\n",
       "         'duterte': 5,\n",
       "         'reding': 3,\n",
       "         'chain': 32,\n",
       "         'manual': 8,\n",
       "         'occupations': 1,\n",
       "         'shiek': 1,\n",
       "         'servants': 19,\n",
       "         'debunks': 3,\n",
       "         'sumerian': 1,\n",
       "         '1920s': 4,\n",
       "         'dioceses': 1,\n",
       "         'isurround': 1,\n",
       "         'ayoreo': 2,\n",
       "         'development': 90,\n",
       "         'liz': 2,\n",
       "         'shigir': 1,\n",
       "         'nouri': 3,\n",
       "         'schindler': 9,\n",
       "         'exaggerating': 5,\n",
       "         'legendary': 10,\n",
       "         'ascent': 1,\n",
       "         'suffered': 38,\n",
       "         'counterattack': 1,\n",
       "         '\\\\r\\\\ncapable': 1,\n",
       "         'constitutional\\\\ncourt': 1,\n",
       "         'budweiser': 3,\n",
       "         'blight': 1,\n",
       "         'hydrophones': 1,\n",
       "         'murder': 284,\n",
       "         'aremote': 1,\n",
       "         'students': 247,\n",
       "         'upvoted': 2,\n",
       "         'slight': 2,\n",
       "         'schedule': 13,\n",
       "         'staple': 2,\n",
       "         'relocates': 1,\n",
       "         'berthold': 1,\n",
       "         'seatbelt': 1,\n",
       "         'galbraith': 1,\n",
       "         'scoops': 1,\n",
       "         'radius': 1,\n",
       "         'rings': 12,\n",
       "         'wilde': 1,\n",
       "         '268': 2,\n",
       "         'improvised': 1,\n",
       "         'greenback': 1,\n",
       "         'isiding': 1,\n",
       "         'isometimes': 4,\n",
       "         'turns': 105,\n",
       "         'comprehensive': 13,\n",
       "         'recant': 1,\n",
       "         'cutters': 1,\n",
       "         'playstation': 4,\n",
       "         'reinstate': 1,\n",
       "         'snowden/nsa': 1,\n",
       "         'denmark\\\\': 1,\n",
       "         'vents': 2,\n",
       "         'maw': 1,\n",
       "         'dazzling': 1,\n",
       "         'accommodation': 3,\n",
       "         'danish': 90,\n",
       "         'diplomat': 83,\n",
       "         'profiles': 5,\n",
       "         'philippa': 1,\n",
       "         'speechand': 1,\n",
       "         'south': 929,\n",
       "         'lockup': 1,\n",
       "         'shrapnel': 4,\n",
       "         'health': 340,\n",
       "         'shoal': 3,\n",
       "         'shrewd': 2,\n",
       "         'nono': 1,\n",
       "         'gadgets': 2,\n",
       "         'typhoid': 2,\n",
       "         'denuclearisation': 1,\n",
       "         'faces': 215,\n",
       "         'delays': 11,\n",
       "         'congregation': 1,\n",
       "         'informant': 18,\n",
       "         'congo': 106,\n",
       "         'amicrobiologist': 1,\n",
       "         'ilyumzhinov': 1,\n",
       "         'vodafone': 8,\n",
       "         'gibbon': 1,\n",
       "         'bhopal': 15,\n",
       "         'eye': 56,\n",
       "         'leatherback': 2,\n",
       "         'smacks': 2,\n",
       "         'decomposed': 1,\n",
       "         'amarkets': 1,\n",
       "         'appreciate': 1,\n",
       "         'swaps': 4,\n",
       "         'dethroned': 1,\n",
       "         'anaesthetic': 3,\n",
       "         'intervention': 58,\n",
       "         'aegean': 3,\n",
       "         'washroom': 2,\n",
       "         'blogs': 9,\n",
       "         'portrait': 7,\n",
       "         'fulvio': 1,\n",
       "         'tamale': 1,\n",
       "         'sophistication': 2,\n",
       "         'dispossessed': 1,\n",
       "         'despicable': 4,\n",
       "         'skinned': 8,\n",
       "         'displacement': 5,\n",
       "         'isaddams': 1,\n",
       "         'greenwich': 1,\n",
       "         'plaa': 1,\n",
       "         'herded': 2,\n",
       "         'beltran': 2,\n",
       "         'shredded': 3,\n",
       "         '1960s': 15,\n",
       "         'recapturing': 1,\n",
       "         'drugrelated': 1,\n",
       "         'platforms': 5,\n",
       "         'pete': 4,\n",
       "         'nothe': 580,\n",
       "         'whisked': 1,\n",
       "         'blowholes': 1,\n",
       "         'virunga': 5,\n",
       "         'abdicated': 1,\n",
       "         'drought': 66,\n",
       "         'lockdown': 7,\n",
       "         'paralyzed': 16,\n",
       "         'geek': 2,\n",
       "         'littered': 5,\n",
       "         'gary': 11,\n",
       "         'capital\\\\n': 1,\n",
       "         'hibernation': 2,\n",
       "         'aiding': 13,\n",
       "         'snapping': 3,\n",
       "         'orbiting': 4,\n",
       "         'stops': 50,\n",
       "         'jonathan': 6,\n",
       "         'moments': 14,\n",
       "         'eclipse': 6,\n",
       "         'feb': 19,\n",
       "         'answered': 5,\n",
       "         'wagyu': 1,\n",
       "         'driver': 55,\n",
       "         'colombo': 1,\n",
       "         'committees': 3,\n",
       "         'zeidi': 1,\n",
       "         'renounce': 3,\n",
       "         'natalia': 2,\n",
       "         'crazy': 26,\n",
       "         'icelanders': 11,\n",
       "         'remit': 1,\n",
       "         'gibe': 1,\n",
       "         'invoke': 3,\n",
       "         'islamised': 1,\n",
       "         'regular': 26,\n",
       "         'kristof': 1,\n",
       "         'aftershocks': 1,\n",
       "         'backdoors': 6,\n",
       "         'perez': 5,\n",
       "         'modernized': 1,\n",
       "         'perch': 1,\n",
       "         'compulsory': 27,\n",
       "         'vacate': 3,\n",
       "         'doctored': 2,\n",
       "         'unknowingly': 4,\n",
       "         'laboratories': 2,\n",
       "         'allagui': 1,\n",
       "         'spenders': 2,\n",
       "         'utility': 12,\n",
       "         'transform': 15,\n",
       "         'creditors': 2,\n",
       "         'condolences': 6,\n",
       "         'backgrounder': 1,\n",
       "         'asians': 9,\n",
       "         'poof': 1,\n",
       "         'libcom': 1,\n",
       "         'antigay': 4,\n",
       "         'loneliest': 3,\n",
       "         'horrified': 3,\n",
       "         'party': 512,\n",
       "         'dans': 1,\n",
       "         'pollutants': 5,\n",
       "         'fees': 38,\n",
       "         'footballing': 1,\n",
       "         'rights': 859,\n",
       "         'stopped': 78,\n",
       "         'famine': 28,\n",
       "         'sanctions': 227,\n",
       "         'sees': 70,\n",
       "         'demonstrator': 4,\n",
       "         'pod': 3,\n",
       "         'edifice': 1,\n",
       "         'set': 460,\n",
       "         'swap': 15,\n",
       "         'american\\\\': 1,\n",
       "         'digesting': 1,\n",
       "         '2026': 1,\n",
       "         'aeromexico': 2,\n",
       "         'karun': 1,\n",
       "         'skating': 1,\n",
       "         'companies\\\\nwith': 1,\n",
       "         'delight': 2,\n",
       "         'gibraltar': 8,\n",
       "         'droneship': 1,\n",
       "         'burrows': 2,\n",
       "         'cusco': 1,\n",
       "         '600x': 1,\n",
       "         'blasted': 13,\n",
       "         'exemption': 6,\n",
       "         'gulls': 1,\n",
       "         'mistrusts': 1,\n",
       "         'frontpages': 1,\n",
       "         'varoufakis': 4,\n",
       "         '252': 3,\n",
       "         'fibrosis': 2,\n",
       "         'internets': 2,\n",
       "         'mobster': 5,\n",
       "         'arrogant': 5,\n",
       "         'siemoniak': 1,\n",
       "         'nismans': 1,\n",
       "         'tao': 2,\n",
       "         '24': 134,\n",
       "         'islamism': 2,\n",
       "         'turbine': 4,\n",
       "         'reconquers': 1,\n",
       "         'ph': 6,\n",
       "         'fugitives': 8,\n",
       "         'wangari': 1,\n",
       "         'belong': 17,\n",
       "         'drummond': 1,\n",
       "         'goddamnit': 1,\n",
       "         'poorest': 47,\n",
       "         'regin': 2,\n",
       "         'trilemma': 1,\n",
       "         'effortlessly': 1,\n",
       "         'leprosy': 1,\n",
       "         'matching': 3,\n",
       "         'derailment': 1,\n",
       "         'female': 260,\n",
       "         'hearth': 1,\n",
       "         'sanity': 2,\n",
       "         'kiselyov': 1,\n",
       "         'inexorably': 2,\n",
       "         'shoulders': 3,\n",
       "         'jumbo': 1,\n",
       "         'insecticide': 7,\n",
       "         'holden': 1,\n",
       "         'drunken': 11,\n",
       "         'undergone': 2,\n",
       "         'distress': 5,\n",
       "         'wordsearch': 1,\n",
       "         'consolidated': 1,\n",
       "         'wikilieaks': 1,\n",
       "         'iain': 1,\n",
       "         'metrojet': 1,\n",
       "         'ferals': 1,\n",
       "         'endgame': 4,\n",
       "         '600/year': 1,\n",
       "         'told': 317,\n",
       "         'yard': 23,\n",
       "         'cruise': 49,\n",
       "         'pleads': 21,\n",
       "         'remembering': 5,\n",
       "         'iswine': 10,\n",
       "         'hamass': 1,\n",
       "         'homelessness': 6,\n",
       "         'powerpoint': 2,\n",
       "         'history': 271,\n",
       "         '120000km2': 1,\n",
       "         'uncontrollably': 1,\n",
       "         'templo': 1,\n",
       "         'francis': 123,\n",
       "         'muslim/white': 1,\n",
       "         'amps': 2,\n",
       "         'extract': 11,\n",
       "         'blackballed': 1,\n",
       "         'rounds': 12,\n",
       "         'malnourishment': 1,\n",
       "         'brasse': 1,\n",
       "         'dengue': 6,\n",
       "         'chan': 4,\n",
       "         'magazine': 49,\n",
       "         'unproven': 1,\n",
       "         'fainthearted': 1,\n",
       "         '\\\\r\\\\nlimit': 1,\n",
       "         'protocols': 3,\n",
       "         'topples': 3,\n",
       "         'vyacheslav': 2,\n",
       "         'spanning': 5,\n",
       "         'retaking': 2,\n",
       "         'prachi': 1,\n",
       "         'consulted': 4,\n",
       "         'lifesaving': 2,\n",
       "         'mbp': 1,\n",
       "         'symbolises': 1,\n",
       "         'vws': 1,\n",
       "         'hilarious': 3,\n",
       "         'footbal': 1,\n",
       "         'chicas': 1,\n",
       "         'invades': 7,\n",
       "         'vocabulary': 1,\n",
       "         'labyrinth': 1,\n",
       "         'taxpayer': 19,\n",
       "         'ilam': 1,\n",
       "         'microbial': 1,\n",
       "         'increment': 1,\n",
       "         'haul': 19,\n",
       "         'collier': 2,\n",
       "         'joker': 2,\n",
       "         'zenroren': 1,\n",
       "         'homewrecking': 1,\n",
       "         'uncover': 18,\n",
       "         'haman': 1,\n",
       "         'erradication': 1,\n",
       "         'imminent': 28,\n",
       "         'orbits': 2,\n",
       "         'downs': 3,\n",
       "         'churkin': 1,\n",
       "         'antiretroviral': 1,\n",
       "         'lazar': 1,\n",
       "         'scotland': 134,\n",
       "         'cathedral': 14,\n",
       "         'scuppered': 1,\n",
       "         'civil': 191,\n",
       "         'aggregate': 1,\n",
       "         'claimed': 91,\n",
       "         'draw': 26,\n",
       "         'resonant': 1,\n",
       "         'inspired': 20,\n",
       "         'tisa': 7,\n",
       "         'worsened': 5,\n",
       "         'schneider': 1,\n",
       "         '540': 3,\n",
       "         'cheating': 23,\n",
       "         '550': 9,\n",
       "         'sunk': 16,\n",
       "         'measurements': 2,\n",
       "         'serhiy': 1,\n",
       "         'sung': 3,\n",
       "         'provides': 30,\n",
       "         'complaint': 27,\n",
       "         'warplane': 7,\n",
       "         'chaotic': 5,\n",
       "         'sweeney': 1,\n",
       "         'truffles': 1,\n",
       "         'calling': 181,\n",
       "         'plaintiff': 1,\n",
       "         '500mn': 1,\n",
       "         'hooliganism': 4,\n",
       "         'cyberespionage': 2,\n",
       "         'warplanes': 44,\n",
       "         'fra': 1,\n",
       "         'lesbos': 1,\n",
       "         'defer': 3,\n",
       "         'misuse': 11,\n",
       "         'abosolve': 1,\n",
       "         'satnavs': 1,\n",
       "         'sailboat': 1,\n",
       "         'fails\\\\': 1,\n",
       "         'earthlings': 1,\n",
       "         'rebut': 1,\n",
       "         '1917': 5,\n",
       "         'prevent': 142,\n",
       "         'annual': 67,\n",
       "         'sacks': 10,\n",
       "         'tripping': 1,\n",
       "         'forensics': 1,\n",
       "         '19th': 13,\n",
       "         'hamas': 364,\n",
       "         'braved': 1,\n",
       "         'hou': 1,\n",
       "         'basile': 1,\n",
       "         '360': 5,\n",
       "         'lifeguards': 1,\n",
       "         '~': 6,\n",
       "         'commander': 106,\n",
       "         'kadima': 5,\n",
       "         'credibility': 17,\n",
       "         'autistic': 6,\n",
       "         'extensively': 1,\n",
       "         'staking': 1,\n",
       "         'override': 4,\n",
       "         'spotting': 3,\n",
       "         'unacceptably': 1,\n",
       "         'physicists': 15,\n",
       "         'rolando': 1,\n",
       "         'disorganised': 1,\n",
       "         'baden': 2,\n",
       "         'poshness': 1,\n",
       "         'istaggering': 2,\n",
       "         'truck': 39,\n",
       "         'motherfuckers': 2,\n",
       "         'heaviest': 8,\n",
       "         '\\\\r\\\\nsaid': 1,\n",
       "         'capitalization': 1,\n",
       "         'notells': 1,\n",
       "         'canty': 1,\n",
       "         'brokering': 1,\n",
       "         'graft': 16,\n",
       "         'sadhvi': 1,\n",
       "         'truth': 87,\n",
       "         'reggio': 1,\n",
       "         'wrapping': 1,\n",
       "         'uncharged': 1,\n",
       "         'ram': 2,\n",
       "         'amay': 20,\n",
       "         'advocated': 1,\n",
       "         'bleach': 3,\n",
       "         'friendliest': 2,\n",
       "         'globally': 18,\n",
       "         'gifts': 9,\n",
       "         'rename': 1,\n",
       "         'jihadism': 1,\n",
       "         'slum': 6,\n",
       "         'virtual': 11,\n",
       "         'persecuting': 1,\n",
       "         'g3': 1,\n",
       "         'vegetarianism': 3,\n",
       "         'colombia': 78,\n",
       "         '290m': 2,\n",
       "         'soil': 38,\n",
       "         'elected': 69,\n",
       "         'dark': 62,\n",
       "         'conakry': 1,\n",
       "         'donut': 1,\n",
       "         'cafferkey': 2,\n",
       "         '4000': 8,\n",
       "         'revives': 6,\n",
       "         'poppy': 12,\n",
       "         'inter': 11,\n",
       "         'tathagata': 1,\n",
       "         'obscenity': 5,\n",
       "         'notest': 2,\n",
       "         'carmelo': 1,\n",
       "         'ecuadorans': 3,\n",
       "         'abductions': 4,\n",
       "         'judiciously': 1,\n",
       "         'socialized': 1,\n",
       "         'istealing': 3,\n",
       "         'nugget': 3,\n",
       "         'eda': 1,\n",
       "         'yasi': 2,\n",
       "         'dakar': 1,\n",
       "         'semitic': 17,\n",
       "         'sclerosis': 4,\n",
       "         'berets': 1,\n",
       "         'cleaning': 18,\n",
       "         'prevailed': 1,\n",
       "         'rosanna': 1,\n",
       "         'arrival': 11,\n",
       "         'sexually': 67,\n",
       "         'turkey\\\\': 1,\n",
       "         'raiding': 4,\n",
       "         'granting': 14,\n",
       "         'profiteer': 1,\n",
       "         'lawrence': 7,\n",
       "         'multinationals': 7,\n",
       "         'tanzania': 26,\n",
       "         'rejoice': 4,\n",
       "         'paedos': 2,\n",
       "         'amark': 1,\n",
       "         'ending': 50,\n",
       "         '192': 6,\n",
       "         'rejecting': 17,\n",
       "         'wartime': 20,\n",
       "         'baher': 1,\n",
       "         'lennon': 1,\n",
       "         'winnipeg': 1,\n",
       "         'protesting': 84,\n",
       "         'awakens': 1,\n",
       "         'rakuten': 1,\n",
       "         'uninhabitable': 5,\n",
       "         'wight': 1,\n",
       "         'bucknell': 1,\n",
       "         '90': 129,\n",
       "         'delivered': 32,\n",
       "         'exhausting': 1,\n",
       "         'scrutinized': 1,\n",
       "         'boo': 4,\n",
       "         'tate': 1,\n",
       "         'pledged': 19,\n",
       "         'canary': 4,\n",
       "         '\\\\r\\\\nhave': 1,\n",
       "         'fled': 48,\n",
       "         '2001': 30,\n",
       "         'burning': 106,\n",
       "         'ample': 1,\n",
       "         '65th': 1,\n",
       "         'fanfare': 2,\n",
       "         'berta': 7,\n",
       "         'uninterrupted': 1,\n",
       "         'amckinney': 1,\n",
       "         'payoff': 2,\n",
       "         'bbc\\\\': 3,\n",
       "         'saddi': 1,\n",
       "         'santiagos': 1,\n",
       "         'complementary': 1,\n",
       "         'inflicted': 9,\n",
       "         'logically': 1,\n",
       "         'hangman\\\\': 1,\n",
       "         'misses': 5,\n",
       "         'zeits': 1,\n",
       "         'tourists\\\\': 1,\n",
       "         'diving': 7,\n",
       "         'emirs': 1,\n",
       "         'macedonias': 1,\n",
       "         'rattles': 3,\n",
       "         'underreported': 2,\n",
       "         'suitable': 5,\n",
       "         'suu': 32,\n",
       "         'hitlers': 5,\n",
       "         'overrespond': 1,\n",
       "         'mediator': 1,\n",
       "         'professor': 58,\n",
       "         'flotilla\\\\nflotilla': 1,\n",
       "         'maiduguri': 3,\n",
       "         'harriet': 2,\n",
       "         'anuses': 1,\n",
       "         'unveils': 44,\n",
       "         'glow': 2,\n",
       "         'powerless': 8,\n",
       "         'clerics': 39,\n",
       "         'godfathers': 1,\n",
       "         'pride': 40,\n",
       "         'carpet': 6,\n",
       "         'aboriginals': 6,\n",
       "         'unviable': 1,\n",
       "         'amicrosoft': 1,\n",
       "         'misunderstood': 2,\n",
       "         'director\\\\': 1,\n",
       "         'sudanese': 34,\n",
       "         'scammers': 3,\n",
       "         'visualization': 2,\n",
       "         'dense': 4,\n",
       "         'niqabitches': 1,\n",
       "         'environments': 1,\n",
       "         'alike\\\\': 1,\n",
       "         'stormed': 22,\n",
       "         'read': 66,\n",
       "         '511': 2,\n",
       "         'pubs': 3,\n",
       "         'learners': 1,\n",
       "         'apcs': 2,\n",
       "         ...})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word_embed](resources/wordvectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to use Glove embeddings to initialize our weights while designing our neural network. Let's load the same so that we can ensure our headline corpus' vocabulary matches where possible with Glove Embedding vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 2196016\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe's embeddings\n",
    "embeddings_index = {}\n",
    "with open('/storage/glove.840B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is not necessary that we will have embeddings for all the words in Glove. So to limit such cases by limiting vocabulary by applying simple logic:  Remove the words that are \"rare\" and are not available in Glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
    "\n",
    "# Define threshold\n",
    "threshold = 10\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31295"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique Words: 36311\n",
      "Number of Words we will use: 31297\n",
      "Percent of Words we will use: 86.19%\n"
     ]
    }
   ],
   "source": [
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total Number of Unique Words:\", len(word_counts))\n",
    "print(\"Number of Words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of Words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the words which are common within headlines but are absent in Glove corpus, we will have to randomly initialize them. Over the training, those values will be finetuned along with those of Glove vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31297\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match GloVe's vectors.\n",
    "embedding_dim = 300\n",
    "\n",
    "nb_words = len(vocab_to_int)\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in GloVe, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the word sequences to equivalent integer sequences so that it can be used as input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 616686\n",
      "Total number of UNKs in headlines: 7139\n",
      "Percent of words that are UNK: 1.16%\n"
     ]
    }
   ],
   "source": [
    "# Change the text from words to integers\n",
    "# If word is not in vocab, replace it with <UNK> (unknown)\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "headlines_sequence = []\n",
    "\n",
    "for daily_headline in clean_headlines:\n",
    "    daily_headlines_seq = []\n",
    "    for headline in daily_headline:\n",
    "        headline_seq = []\n",
    "        for word in headline.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                headline_seq.append(vocab_to_int[word])\n",
    "            else:\n",
    "                headline_seq.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        daily_headlines_seq.append(headline_seq)\n",
    "    headlines_sequence.append(daily_headlines_seq)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[8360,\n",
       "   20565,\n",
       "   19256,\n",
       "   19168,\n",
       "   17819,\n",
       "   20828,\n",
       "   15764,\n",
       "   12124,\n",
       "   11032,\n",
       "   19408,\n",
       "   25123,\n",
       "   2789,\n",
       "   18631,\n",
       "   6474,\n",
       "   17388,\n",
       "   12559,\n",
       "   22398,\n",
       "   8484,\n",
       "   26502,\n",
       "   27409],\n",
       "  [16862,\n",
       "   14562,\n",
       "   901,\n",
       "   16737,\n",
       "   31295,\n",
       "   11377,\n",
       "   6836,\n",
       "   17685,\n",
       "   17400,\n",
       "   12268,\n",
       "   12889,\n",
       "   29411,\n",
       "   9522,\n",
       "   29411,\n",
       "   15354,\n",
       "   26168,\n",
       "   20363,\n",
       "   14140,\n",
       "   25118],\n",
       "  [27739, 17704, 901, 8984, 16932, 1061, 27865],\n",
       "  [14686,\n",
       "   9139,\n",
       "   19757,\n",
       "   27910,\n",
       "   3303,\n",
       "   27874,\n",
       "   25047,\n",
       "   11990,\n",
       "   29231,\n",
       "   15065,\n",
       "   22813,\n",
       "   20457,\n",
       "   27166,\n",
       "   3110,\n",
       "   6341,\n",
       "   15917,\n",
       "   9139,\n",
       "   21947,\n",
       "   18701],\n",
       "  [30931, 13051, 7358, 29302, 22960, 13349, 13000, 6569, 25169, 28630],\n",
       "  [13142, 2979, 1432, 23369, 6350, 29519, 27270, 11725],\n",
       "  [2111, 12164, 4539, 15399, 18032, 22733, 21772, 7266],\n",
       "  [3357, 25529, 27909, 2136, 20726, 16722, 28127, 503],\n",
       "  [2389, 5401, 17577, 20263, 29717, 30951, 8034],\n",
       "  [8479,\n",
       "   3025,\n",
       "   12825,\n",
       "   13041,\n",
       "   3992,\n",
       "   17954,\n",
       "   4865,\n",
       "   15724,\n",
       "   30017,\n",
       "   25012,\n",
       "   31100,\n",
       "   901],\n",
       "  [15871, 11942, 6387, 27551, 25560, 29171, 2445],\n",
       "  [4147,\n",
       "   1617,\n",
       "   19582,\n",
       "   16127,\n",
       "   17165,\n",
       "   2044,\n",
       "   18649,\n",
       "   23511,\n",
       "   16828,\n",
       "   10855,\n",
       "   6367,\n",
       "   4750,\n",
       "   12753],\n",
       "  [8404,\n",
       "   6282,\n",
       "   1247,\n",
       "   14475,\n",
       "   17431,\n",
       "   24302,\n",
       "   18108,\n",
       "   2893,\n",
       "   8064,\n",
       "   8555,\n",
       "   8479,\n",
       "   2979,\n",
       "   15882,\n",
       "   30264,\n",
       "   21442,\n",
       "   24497,\n",
       "   901,\n",
       "   24735,\n",
       "   29302,\n",
       "   12613,\n",
       "   13516],\n",
       "  [8737, 30727, 2875, 6599, 6554, 26859, 10706],\n",
       "  [3422, 26665, 17425, 16632, 7098, 28562, 11620, 1068, 15181],\n",
       "  [2366, 19794, 8469, 27808, 9805, 29530, 16490, 70, 3385, 1693],\n",
       "  [22912, 16023, 12753, 3723, 3511, 18528, 30906, 16485, 13548, 28745, 19667],\n",
       "  [19757,\n",
       "   14995,\n",
       "   12547,\n",
       "   26476,\n",
       "   15917,\n",
       "   27551,\n",
       "   13618,\n",
       "   13631,\n",
       "   10052,\n",
       "   24205,\n",
       "   13733,\n",
       "   15871,\n",
       "   28179,\n",
       "   1298],\n",
       "  [13138, 7098, 28562, 11620, 25776, 29901, 1068, 15181, 19814, 7115, 9014],\n",
       "  [3134, 16272, 418, 3784, 763, 16061, 24153, 2973, 4648, 16964],\n",
       "  [15845,\n",
       "   28952,\n",
       "   15338,\n",
       "   30837,\n",
       "   28745,\n",
       "   8479,\n",
       "   25142,\n",
       "   20355,\n",
       "   10404,\n",
       "   13512,\n",
       "   31295,\n",
       "   11777,\n",
       "   28979,\n",
       "   12241,\n",
       "   15679,\n",
       "   29168,\n",
       "   23500,\n",
       "   11174,\n",
       "   13512,\n",
       "   16905,\n",
       "   6454,\n",
       "   23416,\n",
       "   3353,\n",
       "   12825,\n",
       "   20355,\n",
       "   20123],\n",
       "  [8479,\n",
       "   27682,\n",
       "   18032,\n",
       "   17067,\n",
       "   11993,\n",
       "   3990,\n",
       "   19471,\n",
       "   21380,\n",
       "   29264,\n",
       "   21397,\n",
       "   11697,\n",
       "   21816,\n",
       "   22733,\n",
       "   25178,\n",
       "   9542,\n",
       "   14185,\n",
       "   21397],\n",
       "  [11758,\n",
       "   20807,\n",
       "   20986,\n",
       "   11427,\n",
       "   21036,\n",
       "   13883,\n",
       "   23900,\n",
       "   11758,\n",
       "   28663,\n",
       "   21036,\n",
       "   1389,\n",
       "   15352,\n",
       "   11613,\n",
       "   21678,\n",
       "   13883,\n",
       "   23900,\n",
       "   16727,\n",
       "   29321,\n",
       "   20921,\n",
       "   28935,\n",
       "   24912,\n",
       "   26164,\n",
       "   14089,\n",
       "   9393,\n",
       "   4777,\n",
       "   19582,\n",
       "   29536,\n",
       "   7818],\n",
       "  [18389, 901, 15349, 2939, 27551, 3110, 4164, 16718, 28018, 20418, 15603],\n",
       "  [26648, 24014, 28042, 6921, 31188, 28127, 2979, 13726, 28397, 11843]]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_sequence[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that the variations in the number of news headlines each day and length of each headlines are handled by taking an average number of headlines each day and average length per headline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the length of headlines\n",
    "lengths = []\n",
    "for headlines in headlines_sequence:\n",
    "    for headline in headlines:\n",
    "        lengths.append(len(headline))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49693.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.409917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.789827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             counts\n",
       "count  49693.000000\n",
       "mean      12.409917\n",
       "std        6.789827\n",
       "min        1.000000\n",
       "25%        7.000000\n",
       "50%       10.000000\n",
       "75%       16.000000\n",
       "max       41.000000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit the length of a day's news to 200 words, and the length of any headline to 16 words. These values are chosen to not have an excessively long training time and balance the number of headlines used and the number of words from each headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_headline_length = 16\n",
    "max_daily_length = 200\n",
    "pad_headlines = []\n",
    "\n",
    "# For each date in all the dates available\n",
    "for headlines in headlines_sequence:\n",
    "    pad_daily_headlines = []\n",
    "    # for each headline for each date\n",
    "    for headline in headlines:\n",
    "        # Add headline if it is less than max length\n",
    "        if len(headline) <= max_headline_length:\n",
    "            for word in headline:\n",
    "                pad_daily_headlines.append(word)\n",
    "        # Limit headline if it is more than max length  \n",
    "        else:\n",
    "            headline = headline[:max_headline_length]\n",
    "            for word in headline:\n",
    "                pad_daily_headlines.append(word)\n",
    "    \n",
    "    # Pad daily_headlines if they are less than max length\n",
    "    if len(pad_daily_headlines) < max_daily_length:\n",
    "        for i in range(max_daily_length-len(pad_daily_headlines)):\n",
    "            pad = vocab_to_int[\"<PAD>\"]\n",
    "            pad_daily_headlines.append(pad)\n",
    "    # Limit daily_headlines if they are more than max length\n",
    "    else:\n",
    "        pad_daily_headlines = pad_daily_headlines[:max_daily_length]\n",
    "    pad_headlines.append(pad_daily_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and testing sets.\n",
    "## Validating data will be created during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(pad_headlines, norm_price, test_size = 0.15, random_state = 2)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CNN-RNN architecture\n",
    "![cnn-rnn](resources/cnn-1d-rnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_length = 5\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "weights = initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=2)\n",
    "nb_filter = 16\n",
    "rnn_output_size = 128\n",
    "hidden_dims = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1 - Embedding\n",
    "    model.add(Embedding(nb_words, \n",
    "                         embedding_dim,\n",
    "                         weights=[word_embedding_matrix], \n",
    "                         input_length=max_daily_length))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    # Layer 2 - Convolution 1 with dropout\n",
    "    model.add(Convolution1D(filters = nb_filter, \n",
    "                             kernel_size = filter_length, \n",
    "                             padding = 'same',\n",
    "                             activation = 'relu'))\n",
    "    model.add(Dropout(dropout))    \n",
    "\n",
    "    # Layer 3 - Convolution 2 with Dropout \n",
    "    model.add(Convolution1D(filters = nb_filter, \n",
    "                                 kernel_size = filter_length, \n",
    "                                 padding = 'same',\n",
    "                                 activation = 'relu'))\n",
    "    model.add(Dropout(dropout))    \n",
    "\n",
    "    # Layer 4 - RNN with dropout\n",
    "    model.add(LSTM(rnn_output_size, \n",
    "                    activation=None,\n",
    "                    kernel_initializer=weights,\n",
    "                    dropout = dropout))    \n",
    "\n",
    "    # Layer 5 - Dense FFN with Dropout\n",
    "    model.add(Dense(hidden_dims, kernel_initializer=weights))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(1, \n",
    "                    kernel_initializer = weights,\n",
    "                    name='output'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(lr=learning_rate,clipvalue=1.0))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current model: LR=0.001, Dropout=0.3\n",
      "\n",
      "Train on 1435 samples, validate on 254 samples\n",
      "Epoch 1/100\n",
      "1435/1435 [==============================] - 7s 5ms/step - loss: 0.0577 - val_loss: 0.0516\n",
      "Epoch 2/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0197 - val_loss: 0.0313\n",
      "Epoch 3/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0164 - val_loss: 0.0176\n",
      "Epoch 4/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0134 - val_loss: 0.0167\n",
      "Epoch 5/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0104 - val_loss: 0.0142\n",
      "Epoch 6/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 7/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0090 - val_loss: 0.0108\n",
      "Epoch 8/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0084 - val_loss: 0.0099\n",
      "Epoch 9/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0085 - val_loss: 0.0089\n",
      "Epoch 10/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 11/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0076 - val_loss: 0.0081\n",
      "Epoch 12/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 13/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0076 - val_loss: 0.0085\n",
      "Epoch 14/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 15/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0075 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 16/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0074 - val_loss: 0.0077\n",
      "Epoch 17/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0074 - val_loss: 0.0078\n",
      "Epoch 00017: early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 300)          9389100   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 200, 16)           24016     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 200, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 200, 16)           1296      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 200, 16)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               74240     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 9,505,293\n",
      "Trainable params: 9,505,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Current model: LR=0.001, Dropout=0.5\n",
      "\n",
      "Train on 1435 samples, validate on 254 samples\n",
      "Epoch 1/100\n",
      "1435/1435 [==============================] - 7s 5ms/step - loss: 0.0710 - val_loss: 0.0477\n",
      "Epoch 2/100\n",
      "1435/1435 [==============================] - 4s 3ms/step - loss: 0.0320 - val_loss: 0.0361\n",
      "Epoch 3/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0186 - val_loss: 0.0205\n",
      "Epoch 4/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0155 - val_loss: 0.0104\n",
      "Epoch 5/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0120 - val_loss: 0.0096\n",
      "Epoch 6/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0114 - val_loss: 0.0090\n",
      "Epoch 7/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0106 - val_loss: 0.0074\n",
      "Epoch 8/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0105 - val_loss: 0.0074\n",
      "Epoch 9/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0102 - val_loss: 0.0079\n",
      "Epoch 10/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0097 - val_loss: 0.0075\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 11/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0099 - val_loss: 0.0074\n",
      "Epoch 12/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0095 - val_loss: 0.0072\n",
      "Epoch 13/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0098 - val_loss: 0.0073\n",
      "Epoch 14/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0097 - val_loss: 0.0073\n",
      "Epoch 15/100\n",
      "1435/1435 [==============================] - 4s 3ms/step - loss: 0.0097 - val_loss: 0.0073\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 16/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0096 - val_loss: 0.0073\n",
      "Epoch 17/100\n",
      "1435/1435 [==============================] - 5s 3ms/step - loss: 0.0096 - val_loss: 0.0073\n",
      "Epoch 00017: early stopping\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 200, 300)          9389100   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 200, 16)           24016     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 200, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 200, 16)           1296      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 200, 16)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               74240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 9,505,293\n",
      "Trainable params: 9,505,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "print()\n",
    "save_best_weights = 'best_weights.h5'\n",
    "\n",
    "callbacks = [ModelCheckpoint(save_best_weights, monitor='val_loss', save_best_only=True),\n",
    "            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto'),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=3)]\n",
    "\n",
    "history = model.fit([x_train],\n",
    "                    y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.15,\n",
    "                    verbose=True,\n",
    "                    shuffle=True,\n",
    "                    callbacks = callbacks)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 2s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([x_test], verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007034644158361947"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare testing loss to training and validating loss\n",
    "mse(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Revert prediction back to actual scale\n",
    "def unnormalize(price):\n",
    "    '''Revert values to their unnormalized amounts'''\n",
    "    price = price*(max_price-min_price)+min_price\n",
    "    return(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store back-scaled predictions\n",
    "unnorm_predictions = []\n",
    "for pred in predictions:\n",
    "    unnorm_predictions.append(unnormalize(pred))\n",
    "\n",
    "# Store back-scaled actuals\n",
    "unnorm_y_test = []\n",
    "for y in y_test:\n",
    "    unnorm_y_test.append(unnormalize(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.57360821093789"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the median absolute error for the predictions\n",
    "mae(unnorm_y_test, unnorm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    299.000000\n",
       "mean       7.094101\n",
       "std      139.532324\n",
       "min     -673.139648\n",
       "25%      -54.689941\n",
       "50%       10.759766\n",
       "75%       87.465332\n",
       "max      541.050782\n",
       "dtype: float64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(unnorm_y_test).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Your Own Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code necessary to make your own predictions. I found that the predictions are most accurate when there is no padding included in the input data. In the create_news variable, I have some default news that you can use, which is from April 30th, 2017. Just change the text to whatever you want, then see the impact your new headline will have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def news_to_int(news):\n",
    "    '''Convert your created news into integers'''\n",
    "    ints = []\n",
    "    for word in news.split():\n",
    "        if word in vocab_to_int:\n",
    "            ints.append(vocab_to_int[word])\n",
    "        else:\n",
    "            ints.append(vocab_to_int['<UNK>'])\n",
    "    return ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding_news(news):\n",
    "    '''Adjusts the length of your created news to fit the model's input values.'''\n",
    "    padded_news = news\n",
    "    if len(padded_news) < max_daily_length:\n",
    "        for i in range(max_daily_length-len(padded_news)):\n",
    "            padded_news.append(vocab_to_int[\"<PAD>\"])\n",
    "    elif len(padded_news) > max_daily_length:\n",
    "        padded_news = padded_news[:max_daily_length]\n",
    "    return padded_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dow should open: -23.75 from the previous open.\n"
     ]
    }
   ],
   "source": [
    "# Default news that you can use\n",
    "\n",
    "create_news =  \"Woman says note from Chinese 'prisoner' was hidden in new purse. \\\n",
    "               21,000 AT&T workers poised for Monday strike \\\n",
    "               housands march against Trump climate policies in D.C., across USA \\\n",
    "               Kentucky judge won't hear gay adoptions because it's not in the child's \\\"best interest\\\" \\\n",
    "               Multiple victims shot in UTC area apartment complex \\\n",
    "               Drones Lead Police to Illegal Dumping in Riverside County | NBC Southern California \\\n",
    "               An 86-year-old Californian woman has died trying to fight a man who was allegedly sexually assaulting her 61-year-old friend. \\\n",
    "               Fyre Festival Named in $5Million+ Lawsuit after Stranding Festival-Goers on Island with Little Food, No Security. \\\n",
    "               The \\\"Greatest Show on Earth\\\" folds its tent for good \\\n",
    "               U.S.-led fight on ISIS have killed 352 civilians: Pentagon \\\n",
    "               Woman offers undercover officer sex for $25 and some Chicken McNuggets \\\n",
    "               Ohio bridge refuses to fall down after three implosion attempts \\\n",
    "               Jersey Shore MIT grad dies in prank falling from library dome \\\n",
    "               New York graffiti artists claim McDonald's stole work for latest burger campaign \\\n",
    "               SpaceX to launch secretive satellite for U.S. intelligence agency \\\n",
    "               Severe Storms Leave a Trail of Death and Destruction Through the U.S. \\\n",
    "               Hamas thanks N. Korea for its support against ‘Israeli occupation’ \\\n",
    "               Baker Police officer arrested for allegedly covering up details in shots fired investigation \\\n",
    "               Miami doctor’s call to broker during baby’s delivery leads to $33.8 million judgment \\\n",
    "               Minnesota man gets 15 years for shooting 5 Black Lives Matter protesters \\\n",
    "               South Australian woman facing possible 25 years in Colombian prison for drug trafficking \\\n",
    "               The Latest: Deal reached on funding government through Sept. \\\n",
    "               Russia flaunts Arctic expansion with new military bases\"\n",
    "\n",
    "clean_news = clean_text(create_news)\n",
    "\n",
    "int_news = news_to_int(clean_news)\n",
    "\n",
    "pad_news = padding_news(int_news)\n",
    "\n",
    "pad_news = np.array(pad_news).reshape((1,-1))\n",
    "\n",
    "pred = model.predict([pad_news])\n",
    "\n",
    "price_change = unnormalize(pred)\n",
    "\n",
    "print(\"The Dow should open: {} from the previous open.\".format(np.round(price_change[0][0],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
