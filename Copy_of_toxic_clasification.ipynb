{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of toxic_clasification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mishra-atul5001/Data-Science-and-ML-insights-Projects/blob/master/Copy_of_toxic_clasification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcusIajAblEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dbfedd23-987f-40f6-9a60-b7f420e5d553"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras import layers,models\n",
        "#import pyautogui\n",
        "import nltk\n",
        "import re\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling1D,Dense, LSTM,GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IytJ1EHhcWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "ee482ad2-bb47-4f20-bf3d-8f3a6531a2b3"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZmaishxblE0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "150b37c6-f248-4f5c-fd4d-93ee5499c3cf"
      },
      "source": [
        "\n",
        "data = pd.read_csv(\"/content/drive/My Drive/toxic_train_2.csv\",encoding = 'ISO-8859-1')\n",
        "\n",
        "data.head(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bbq \\n\\nbe a man and lets discuss it-maybe ove...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Before you start throwing accusations and warn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text  toxic\n",
              "0       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
              "1  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...      0\n",
              "2  bbq \\n\\nbe a man and lets discuss it-maybe ove...      0\n",
              "3  Hey... what is it..\\n@ | talk .\\nWhat is it......      1\n",
              "4  Before you start throwing accusations and warn...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4lJ_vMBVfDT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "fc9b2dd9-c682-43ce-fd58-1ad2eb7e31e2"
      },
      "source": [
        "data.toxic.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    144111\n",
              "1     40307\n",
              "Name: toxic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpv8ZOWqZPwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind=data[data['toxic']==0].index.tolist()\n",
        "ind1=data[data['toxic']==1].index.tolist()\n",
        "data1=data.iloc[ind[0:40307],:]\n",
        "data2=data.iloc[ind1[0:],:]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUHgClA7ZTSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "35c60e62-26ea-46b0-e3f0-84f0de4eb25e"
      },
      "source": [
        "final=data2.append(data1)\n",
        "final['toxic'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    40307\n",
              "0    40307\n",
              "Name: toxic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNv5pmA2blFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0378f05c-9e07-4bb8-cc1a-cfa05b960598"
      },
      "source": [
        "x=final.iloc[:,0].values\n",
        "y=final['toxic'].values\n",
        "print(np.shape(x),np.shape(y))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80614,) (80614,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FsKmC9fZJ9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVJd9ZWoblFE",
        "colab_type": "text"
      },
      "source": [
        "#### As shown above, the \"toxic\" label has 2 prediction classes - \"0\": negative/non-toxic, \"1\": positive/toxic.\n",
        "#### It indicates that training data is highly imbalanced on the prediction classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uZf6_9XblFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lowercase letters\n",
        "for i in range(0,len(x)):\n",
        "    text=x[i]\n",
        "    x[i]=text.lower()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ9UxAf5blFV",
        "colab_type": "text"
      },
      "source": [
        "#### Remove Contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsCuYEOgblFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is / how does\",\n",
        "\"i'd\": \"i had / i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i shall / i will\",\n",
        "\"i'll've\": \"i shall have / i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"}\n",
        "for i in range(0,len(x)):\n",
        "    for j in range(0,len(contractions)):\n",
        "        keys=\"\"+list(contractions.keys())[j]\n",
        "        value=\"\"+list(contractions.values())[j]\n",
        "        if keys in x[i]:\n",
        "            phrase = re.sub(keys, value, x[i])\n",
        "            x[i]=phrase\n",
        "        else:\n",
        "            x[i]=x[i]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T06RlTm8blFe",
        "colab_type": "text"
      },
      "source": [
        "#### Remove Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-o4SfO0blFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "for i in range(0,len(x)):\n",
        "    no_punct = \"\"\n",
        "    for char in x[i]:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "    x[i]=no_punct"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClQqXmiNblFm",
        "colab_type": "text"
      },
      "source": [
        "#### Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyUK0EOeblFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove(list): \n",
        "    pattern = '[0-9]'\n",
        "    list = [re.sub(pattern, '', i) for i in x] \n",
        "    return list\n",
        "x=remove(x)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8eTVqwPblFt",
        "colab_type": "text"
      },
      "source": [
        "#### Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-JOC7rTiR6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "f68d1169-8eab-4ac8-bd7f-d9a60e30c63d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgzgLq3qblFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stopwords=stopwords.words('english')\n",
        "for i in range(0,len(x)):\n",
        "    word_tokens = word_tokenize(x[i]) \n",
        "    sent=\"\"\n",
        "    for char in word_tokens:\n",
        "        if char not in stopwords:\n",
        "            sent=sent+\" \"+char\n",
        "    x[i]=sent\n",
        "            \n",
        "        "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb4QjzwDblF1",
        "colab_type": "text"
      },
      "source": [
        "#### Remove extra spaces "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R4PkGS1blF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,len(x)):\n",
        "    x[i]=\" \".join(x[i].split())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naOuESbNblF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Tokenize"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l15PgR-lbqky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x[0]\n",
        "cleaned_train = x.copy()\n",
        "cleaned_test = y.copy()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqA7NbC0e7rd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "35cb46a1-b390-4927-c325-4c718c513ed9"
      },
      "source": [
        "cleaned_test[4]\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvSg-rMYWOJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv  = CountVectorizer(binary = True) #We put TRUE, because we want all non zero to be 1 as we are looking for Binary Outcomes!\n",
        "cv.fit(cleaned_train)\n",
        "X = cv.transform(cleaned_train)\n",
        "#X_test = cv.transform(cleaned_test)\n",
        "cleaned_test = np.array(cleaned_test).reshape(-1, 1).ravel()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkN73Ezfe4jQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "6774215a-3ea1-4ae1-8a2f-3d67c453abab"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "X_train,X_val,y_train,y_val = train_test_split(X,cleaned_test,train_size = 0.7,random_state = 4)\n",
        "for c in [0.001,0.01,0.05,0.25,0.5,0.75,1]:\n",
        "    lr = LogisticRegression(C = c,max_iter = 300)\n",
        "    lr.fit(X_train,y_train)\n",
        "    print('For C: ', c , 'Accuracy Score: ',round(accuracy_score(y_val,lr.predict(X_val)),4))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For C:  0.001 Accuracy Score:  0.8555\n",
            "For C:  0.01 Accuracy Score:  0.9118\n",
            "For C:  0.05 Accuracy Score:  0.9238\n",
            "For C:  0.25 Accuracy Score:  0.9327\n",
            "For C:  0.5 Accuracy Score:  0.9347\n",
            "For C:  0.75 Accuracy Score:  0.9359\n",
            "For C:  1 Accuracy Score:  0.9364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UQmWU10e4YC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f0128672-a0c4-40fd-a558-6ca964019d81"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# Now let's check for the test set\n",
        "test_model = LogisticRegression(C = 1,max_iter=1000)\n",
        "test_model.fit(X_train,y_train)\n",
        "print('Final Accuracy: ',round(accuracy_score(y_val,test_model.predict(X_val)),2))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Accuracy:  0.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMNhVmrmdG1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "18c628af-818a-4ebd-d4c8-a745312f9484"
      },
      "source": [
        "#y_test_pred=model.predict_classes(x_test)\n",
        "num=2490\n",
        "print('Actual:',y_val[num])\n",
        "\n",
        "x_test1=np.reshape(X_val[num],(1,132390))\n",
        "\n",
        "y_test_pred=test_model.predict(x_test1)\n",
        "print('predicted:',y_test_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: 0\n",
            "predicted: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18KhHwt3TVta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5101a028-bdc0-4710-a982-cf4fb5e8856e"
      },
      "source": [
        "x_test1[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x132390 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 10 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10OVLsyy2cNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#feature_to_coefficient = {word: coef for word,coef in zip(cv.get_feature_names(),test_model.coef_[0])}\n",
        "#feature_to_coefficient"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fbu3mcQig7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "0788c89f-f050-4309-f6ce-2ecbec68bde0"
      },
      "source": [
        "feature_to_coefficient = {word: coef for word,coef in zip(cv.get_feature_names(),test_model.coef_[0])}\n",
        "\n",
        "print('Top 5 Toxic Words are: ')\n",
        "for toxic_words in sorted(feature_to_coefficient.items(),key = lambda x: x[1],reverse = True)[:5]:\n",
        "    print(toxic_words)\n",
        "\n",
        "print('------***********------')\n",
        "\n",
        "print('Top 5 Non Toxic Words are: ')\n",
        "for non_toxic_words in sorted(feature_to_coefficient.items(),key = lambda x: x[1])[:5]:\n",
        "    print(non_toxic_words)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 5 Toxic Words are: \n",
            "('bitch', 6.446801173981827)\n",
            "('fuck', 5.9928607921307275)\n",
            "('bitches', 5.853093221524549)\n",
            "('hoes', 5.5876913605179)\n",
            "('fucking', 5.523420721826939)\n",
            "------***********------\n",
            "Top 5 Non Toxic Words are: \n",
            "('redirect', -2.4810725848218307)\n",
            "('ani', -1.9891536768571911)\n",
            "('mailbox', -1.5698647597105921)\n",
            "('apologize', -1.536082963524677)\n",
            "('talkâ', -1.4696822309316884)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ysUHpl5igyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "22459545-a110-432b-aca4-52bb28929325"
      },
      "source": [
        "x[1]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'hey | talk exclusive group wp talibanswho good destroying selfappointed purist gang one asks questions abt antisocial destructive noncontribution wp ask sityush clean behavior issue nonsensical warnings'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_v5-Am7iglF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ngram_cv = CountVectorizer(binary=True,ngram_range=(1,2))\n",
        "ngram_cv.fit(cleaned_train)\n",
        "X = ngram_cv.transform(cleaned_train)\n",
        "#X_test = ngram_cv.transform(cleaned_test_data_from_stop_words)\n",
        "#cleaned_test = np.array(cleaned_test).reshape(-1, 1).ravel()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHbrzcsgk8W8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "4e02e9b1-dd80-4cc8-968b-ca4a95643af5"
      },
      "source": [
        "X_train,X_val,y_train,y_val = train_test_split(X,cleaned_test,train_size = 0.7,random_state = 4)\n",
        "for c in [0.001,0.01,0.05,0.25,0.5,0.75,1]:\n",
        "    lr = LogisticRegression(C = c,max_iter = 300)\n",
        "    lr.fit(X_train,y_train)\n",
        "    print('For C: ', c , 'Accuracy Score: ',round(accuracy_score(y_val,lr.predict(X_val)),4))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For C:  0.001 Accuracy Score:  0.8557\n",
            "For C:  0.01 Accuracy Score:  0.9124\n",
            "For C:  0.05 Accuracy Score:  0.9251\n",
            "For C:  0.25 Accuracy Score:  0.9328\n",
            "For C:  0.5 Accuracy Score:  0.9346\n",
            "For C:  0.75 Accuracy Score:  0.9355\n",
            "For C:  1 Accuracy Score:  0.936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l9od0RRk8UB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "42db543f-35e3-4ca8-f1e2-a8585604e31c"
      },
      "source": [
        "# Now let's check for the test set\n",
        "test_model = LogisticRegression(C = 1,max_iter=1000)\n",
        "test_model.fit(X_train,y_train)\n",
        "print('Final Accuracy: ',round(accuracy_score(y_val,test_model.predict(X_val)),2))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Accuracy:  0.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_zjzhh3QCCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x1='holy shit'\n",
        "#test_model.predict(x1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjKFEPrvk8PW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_me_top_words(model):\n",
        "    feature_to_coefficient = {word: coef for word,coef in zip(cv.get_feature_names(),model.coef_[0])}\n",
        "\n",
        "    print('Top 5 Toxic Words are: ')\n",
        "    for toxic_words in sorted(feature_to_coefficient.items(),key = lambda x: x[1],reverse = True)[:5]:\n",
        "        print(toxic_words)\n",
        "\n",
        "    print('------***********------')\n",
        "\n",
        "    print('Top 5 Non Toxic Words are: ')\n",
        "    for non_toxic_words in sorted(feature_to_coefficient.items(),key = lambda x: x[1])[:5]:\n",
        "        print(non_toxic_words)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f8kqwHxk8KS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "72483d80-d6fe-4e9d-cf65-6497cec2a317"
      },
      "source": [
        "show_me_top_words(test_model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 5 Toxic Words are: \n",
            "('worldvieweg', 6.461775729200974)\n",
            "('yoghurt', 5.5856688165830395)\n",
            "('outcrossing', 3.8626658882875766)\n",
            "('oddball', 3.8547120539757556)\n",
            "('wikimediaprivacy', 3.6311075130655106)\n",
            "------***********------\n",
            "Top 5 Non Toxic Words are: \n",
            "('irrelvance', -1.4400484707651295)\n",
            "('httpenwikipediaorgwikishimingyicitenote', -1.2541835011585512)\n",
            "('kanpur', -1.0979827981093941)\n",
            "('delete', -1.0688916717574548)\n",
            "('larxenes', -0.9735541701618435)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pysOTtyk8HK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "949b9632-6884-431c-a2d7-40838f558abe"
      },
      "source": [
        "#y_test_pred=model.predict_classes(x_test)\n",
        "num=24180\n",
        "print('Actual:',y_val[num])\n",
        "\n",
        "x_test1=np.reshape(X_val[num],(1,1306759))\n",
        "\n",
        "y_test_pred=test_model.predict(x_test1)\n",
        "print('predicted:',y_test_pred)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: 1\n",
            "predicted: [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyK4j3T3k8C7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x1='holy shit'\n",
        "#test_model.predict(instance)\n",
        "#x1=np.reshape(x1,(-1,1))\n",
        "#custom_pred = test_model.predict(datapoint,features = ['x1'])\n",
        "#print('predicted:',custom_pred)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1r9mYXvmTaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}