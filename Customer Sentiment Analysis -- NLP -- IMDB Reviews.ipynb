{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Sentiment Analysis using NLP\n",
    "\n",
    "### Data Set: IMDB Reviews\n",
    "\n",
    "\n",
    "### PART - 1\n",
    "\n",
    "Roadmap we will follow:\n",
    "1. Importing the Data Set\n",
    "2. Processing the DataSet\n",
    "3. Vectorizing the DataSet\n",
    "4. Building a CLassifier for the DataSet to predict the **Sentiment**\n",
    "\n",
    "Note: **The targets/labels we use will be the same for training and testing because both datasets are structured the same, where the first 12.5k are positive and the last 12.5k are negative.** This is the form in which our **full_train and full_test** are made!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re,os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data Set\n",
    "\n",
    "'''We have 2 text files containing all the POSITIVE/NEGATIVE/NEUTRAL reviews. Firstly we have to fetch it into lists,\n",
    "    and then to make sure, there is no space between the reviews, we have to STRIP off the spaces!. We also got the encoding error,\n",
    "    hence had to use universal UTF8 encoding!'''\n",
    "\n",
    "training_set = []\n",
    "for line in open('E:/UpGrad_Data Science/Offile ML Projects/NLP for IMDB Customer Review/movie_data/full_train.txt','r',\n",
    "                 encoding='utf8'):\n",
    "    training_set.append(line.strip())\n",
    "    \n",
    "testing_set = []\n",
    "for line in open('E:/UpGrad_Data Science/Offile ML Projects/NLP for IMDB Customer Review/movie_data/full_test.txt','r',\n",
    "                 encoding='utf8'):\n",
    "    testing_set.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE NIGHT LISTENER (2006) **1/2 Robin Williams, Toni Collette, Bobby Cannavale, Rory Culkin, Joe Morton, Sandra Oh, John Cullum, Lisa Emery, Becky Ann Baker. (Dir: Patrick Stettner) <br /><br />Hitchcockian suspenser gives Williams a stand-out low-key performance.<br /><br />What is it about celebrities and fans? What is the near paranoia one associates with the other and why is it almost the norm? <br /><br />In the latest derange fan scenario, based on true events no less, Williams stars as a talk-radio personality named Gabriel No one, who reads stories he's penned over the airwaves and has accumulated an interesting fan in the form of a young boy named Pete Logand (Culkin) who has submitted a manuscript about the travails of his troubled youth to No one's editor Ashe (Morton) who gives it to No one to read for himself. <br /><br />No one is naturally disturbed but ultimately intrigued about the nightmarish existence of Pete being abducted and sexually abused for years until he was finally rescued by a nurse named Donna (Collette giving an excellent performance) who has adopted the boy but her correspondence with No one reveals that Pete is dying from AIDS. Naturally No one wants to meet the fans but is suddenly in doubt to their possibly devious ulterior motives when the seed is planted by his estranged lover Jess (Cannavale) whose sudden departure from their New York City apartment has No one in an emotional tailspin that has only now grown into a tempest in a teacup when he decides to do some investigating into Donna and Pete's backgrounds discovering some truths that he didn't anticipate.<br /><br />Written by Armistead Maupin (who co-wrote the screenplay with his former lover Terry Anderson and the film's novice director Stettner) and based on a true story about a fan's hoax found out has some Hitchcockian moments that run on full tilt like any good old fashioned pot-boiler does. It helps that Williams gives a stand-out, low-key performance as the conflicted good-hearted personality who genuinely wants to believe that his number one fan is in fact real and does love him (the one thing that has escaped his own reality) and has some unsettling dreadful moments with the creepy Collette whose one physical trait I will leave unmentioned but underlines the desperation of her character that can rattle you to the core.<br /><br />However the film runs out of gas and eventually becomes a bit repetitive and predictable despite a finely directed piece of hoodwink and mystery by Stettner, it pays to listen to your own inner voice: be careful of what you hope for.\n"
     ]
    }
   ],
   "source": [
    "# Let's print atleast one review of each set!\n",
    "print(training_set[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is amazing because the fact that the real people portray themselves and their real life experience and do such a good job it's like they're almost living the past over again. Jia Hongsheng plays himself an actor who quit everything except music and drugs struggling with depression and searching for the meaning of life while being angry at everyone especially the people who care for him most. There's moments in the movie that will make you wanna cry because the family especially the father did such a good job. However, this movie is not for everyone. Many people who suffer from depression will understand Hongsheng's problem and why he does the things he does for example keep himself shut in a dark room or go for walks or bike rides by himself. Others might see the movie as boring because it's just so real that its almost like a documentary. Overall this movie is great and Hongsheng deserved an Oscar for this movie so did his Dad.\n"
     ]
    }
   ],
   "source": [
    "print(testing_set[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that there are punctuations such as **:,',.,br** which have to be removed.   \n",
    "The best way is to remove using **Regular Expressions**, which we are going to do next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_replaced_with_no_space = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "to_be_replaced_with_space = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def process_review(reviews):\n",
    "    reviews = [to_be_replaced_with_no_space.sub('',line.lower()) for line in reviews]\n",
    "    reviews = [to_be_replaced_with_space.sub(' ',line.lower()) for line in reviews]\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's process our Reviews\n",
    "cleaned_train = process_review(training_set)\n",
    "cleaned_test = process_review(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the night listener  **  robin williams toni collette bobby cannavale rory culkin joe morton sandra oh john cullum lisa emery becky ann baker dir patrick stettner  hitchcockian suspenser gives williams a stand out low key performance what is it about celebrities and fans what is the near paranoia one associates with the other and why is it almost the norm  in the latest derange fan scenario based on true events no less williams stars as a talk radio personality named gabriel no one who reads stories hes penned over the airwaves and has accumulated an interesting fan in the form of a young boy named pete logand culkin who has submitted a manuscript about the travails of his troubled youth to no ones editor ashe morton who gives it to no one to read for himself  no one is naturally disturbed but ultimately intrigued about the nightmarish existence of pete being abducted and sexually abused for years until he was finally rescued by a nurse named donna collette giving an excellent performance who has adopted the boy but her correspondence with no one reveals that pete is dying from aids naturally no one wants to meet the fans but is suddenly in doubt to their possibly devious ulterior motives when the seed is planted by his estranged lover jess cannavale whose sudden departure from their new york city apartment has no one in an emotional tailspin that has only now grown into a tempest in a teacup when he decides to do some investigating into donna and petes backgrounds discovering some truths that he didnt anticipate written by armistead maupin who co wrote the screenplay with his former lover terry anderson and the films novice director stettner and based on a true story about a fans hoax found out has some hitchcockian moments that run on full tilt like any good old fashioned pot boiler does it helps that williams gives a stand out low key performance as the conflicted good hearted personality who genuinely wants to believe that his number one fan is in fact real and does love him the one thing that has escaped his own reality and has some unsettling dreadful moments with the creepy collette whose one physical trait i will leave unmentioned but underlines the desperation of her character that can rattle you to the core however the film runs out of gas and eventually becomes a bit repetitive and predictable despite a finely directed piece of hoodwink and mystery by stettner it pays to listen to your own inner voice be careful of what you hope for\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_train[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this movie is amazing because the fact that the real people portray themselves and their real life experience and do such a good job its like theyre almost living the past over again jia hongsheng plays himself an actor who quit everything except music and drugs struggling with depression and searching for the meaning of life while being angry at everyone especially the people who care for him most theres moments in the movie that will make you wanna cry because the family especially the father did such a good job however this movie is not for everyone many people who suffer from depression will understand hongshengs problem and why he does the things he does for example keep himself shut in a dark room or go for walks or bike rides by himself others might see the movie as boring because its just so real that its almost like a documentary overall this movie is great and hongsheng deserved an oscar for this movie so did his dad\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_test[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have cleaned up our DATA for training and Testing, but the one thing that stands in between this data and modelling is the **way** in which we will feed our data to Model.    \n",
    "The ML model will not be able to understand if we provide the dataset as it is, because the model needs a NUMERIC dataframe to process.Now how do we go onto do that, **Bag of Words** is the solution to this using **Vectorizer**.   \n",
    "**Bag of Words splits the document into TOKENS using some sort of pattern. Then the weight of each token is assigned which is proportional to frequency with which it shows up in the document. Then a matrix is formed with each row representing a document and column addressing a token.**\n",
    "\n",
    "We will be using **Count Vectorizer** here as ***it counts the number of times a token shows up and uses that value as weight**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv  = CountVectorizer(binary = True) #We put TRUE, because we want all non zero to be 1 as we are looking for Binary Outcomes!\n",
    "cv.fit(cleaned_train)\n",
    "X = cv.transform(cleaned_train)\n",
    "X_test = cv.transform(cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C:  0.001 Accuracy Score:  0.8423\n",
      "For C:  0.01 Accuracy Score:  0.872\n",
      "For C:  0.05 Accuracy Score:  0.8828\n",
      "For C:  0.25 Accuracy Score:  0.8813\n",
      "For C:  0.5 Accuracy Score:  0.8789\n",
      "For C:  0.75 Accuracy Score:  0.8773\n",
      "For C:  1 Accuracy Score:  0.8745\n"
     ]
    }
   ],
   "source": [
    "target = [1 if i<12500 else 0 for i in range(25000)]\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,target,train_size = 0.7,random_state = 4)\n",
    "for c in [0.001,0.01,0.05,0.25,0.5,0.75,1]:\n",
    "    lr = LogisticRegression(C = c,max_iter = 300)\n",
    "    lr.fit(X_train,y_train)\n",
    "    print('For C: ', c , 'Accuracy Score: ',round(accuracy_score(y_val,lr.predict(X_val)),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy:  0.88144\n"
     ]
    }
   ],
   "source": [
    "# Now let's check for the test set\n",
    "test_model = LogisticRegression(C = 0.05)\n",
    "test_model.fit(X,target)\n",
    "print('Final Accuracy: ',accuracy_score(target,test_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a ML Model with **88% Accuracy** of identifying **Negative and Positive Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Positive and Negative Words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Positive Words are: \n",
      "('excellent', 0.9283544345896336)\n",
      "('perfect', 0.7944277796364583)\n",
      "('great', 0.6745552805685403)\n",
      "('amazing', 0.6164834564379139)\n",
      "('superb', 0.6055919684474416)\n",
      "------***********------\n",
      "Top 5 Negative Words are: \n",
      "('worst', -1.3679897707836337)\n",
      "('waste', -1.1688808928148235)\n",
      "('awful', -1.0273337525366806)\n",
      "('poorly', -0.8748022406393018)\n",
      "('boring', -0.8591221194172675)\n"
     ]
    }
   ],
   "source": [
    "feature_to_coefficient = {word: coef for word,coef in zip(cv.get_feature_names(),test_model.coef_[0])}\n",
    "\n",
    "print('Top 5 Positive Words are: ')\n",
    "for best_positive_words in sorted(feature_to_coefficient.items(),key = lambda x: x[1],reverse = True)[:5]:\n",
    "    print(best_positive_words)\n",
    "\n",
    "print('------***********------')\n",
    "\n",
    "print('Top 5 Negative Words are: ')\n",
    "for best_negative_words in sorted(feature_to_coefficient.items(),key = lambda x: x[1])[:5]:\n",
    "    print(best_negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the above what we did was some basic stuff for NLP Starting, but what we didnt' do was removing **Stop Words** such as **if,he,she,but,we**, we also didn't normalised the words, which we can do using **Stemming and Lemmatizing**. These are the 2 ways in which **plural words are made singular**,we also did not took care of the new words coming up in **test data** which can make us loose valuable information which we will be doing using **ngram range** in our vectorization!    \n",
    "\n",
    "We will also be looking at the number of **Word Counts** as it can give more predictive power to our Model!  \n",
    "Then finally we will be building **Machine Learning Models** such as **SVM** and **Ensembling Technique** to see the result!\n",
    "\n",
    "## Part -2\n",
    "\n",
    "Let's Begin!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will proceed on with removing **Stop Words**.\n",
    "\n",
    "What are stop words?  \n",
    "**Stop Words are very common occuring words like 'he,she,if,is,they' which should be removed(but not always). Removing them helps in improving the Performance of the Model as it shortens the Dimensions by being selective!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_library_of_stop_words = stopwords.words('english')\n",
    "def remove_stop_words(text_input):\n",
    "    removed_stop_words = []\n",
    "    for review in text_input:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in the_library_of_stop_words])\n",
    "        )\n",
    "    return removed_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the stop words from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_data_from_stop_words = remove_stop_words(cleaned_train)\n",
    "cleaned_test_data_from_stop_words = remove_stop_words(cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brilliant acting lesley ann warren best dramatic hobo lady ever seen love scenes clothes warehouse second none corn face classic good anything blazing saddles take lawyers also superb accused turncoat selling boss dishonest lawyer pepto bolt shrugs indifferently im lawyer says three funny words jeffrey tambor favorite later larry sanders show fantastic mad millionaire wants crush ghetto character malevolent usual hospital scene scene homeless invade demolition site time classics look legs scene two big diggers fighting one bleeds movie gets better time see quite often'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train_data_from_stop_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do the modelling again\n",
    "cv = CountVectorizer(binary = True)\n",
    "cv.fit(cleaned_train_data_from_stop_words)\n",
    "X = cv.transform(cleaned_train_data_from_stop_words)\n",
    "X_test = cv.transform(cleaned_test_data_from_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X,target,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at C=0.01: 0.8755\n",
      "Accuracy at C=0.05: 0.884\n",
      "Accuracy at C=0.25: 0.8816\n",
      "Accuracy at C=0.5: 0.8795\n",
      "Accuracy at C=0.75: 0.8776\n"
     ]
    }
   ],
   "source": [
    "for hyper_parameter in [0.01,0.05,0.25,0.5,0.75]:\n",
    "    lr_model = LogisticRegression(C = hyper_parameter,max_iter = 300)\n",
    "    lr_model.fit(X_train,y_train)\n",
    "\n",
    "    print('Accuracy at C=%s: %s'%(hyper_parameter,round(accuracy_score(y_val,lr_model.predict(X_val)),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Accuracy Obtained after removing the **Stop Words**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy:  0.87972\n"
     ]
    }
   ],
   "source": [
    "# Now let's check for the test set\n",
    "test_model = LogisticRegression(C = 0.05)\n",
    "test_model.fit(X,target)\n",
    "print('Final Accuracy: ',accuracy_score(target,test_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_me_top_words(model):\n",
    "    feature_to_coefficient = {word: coef for word,coef in zip(cv.get_feature_names(),model.coef_[0])}\n",
    "\n",
    "    print('Top 5 Positive Words are: ')\n",
    "    for best_positive_words in sorted(feature_to_coefficient.items(),key = lambda x: x[1],reverse = True)[:5]:\n",
    "        print(best_positive_words)\n",
    "\n",
    "    print('------***********------')\n",
    "\n",
    "    print('Top 5 Negative Words are: ')\n",
    "    for best_negative_words in sorted(feature_to_coefficient.items(),key = lambda x: x[1])[:5]:\n",
    "        print(best_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Positive Words are: \n",
      "('excellent', 0.9156937726215434)\n",
      "('perfect', 0.7811217155798308)\n",
      "('great', 0.6583322243450895)\n",
      "('amazing', 0.6290837446397891)\n",
      "('favorite', 0.6256923899072799)\n",
      "------***********------\n",
      "Top 5 Negative Words are: \n",
      "('worst', -1.3848468542026808)\n",
      "('waste', -1.199588607791577)\n",
      "('awful', -1.0409031829500135)\n",
      "('poorly', -0.8863416928527236)\n",
      "('disappointment', -0.8540634899012562)\n"
     ]
    }
   ],
   "source": [
    "show_me_top_words(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatizing!\n",
    "\n",
    "We generally feed a Normalised Data to our Machine Learning Model in order to obtain fair and better results, but here we haven't done that!    \n",
    "**Stemming**: Stemming is one way of Normalizing the Data, where a word is brought to its root word or base line. Ex: 'Asked,Asking,Ask' are all of the same stem **'Ask'**  \n",
    "**Lemmatization**: Lemmatization is process in which the word is brought to it's more root.base form by using a set of conditions in a more calculated process. It resolves words to their dictionary form Ex: 'Asked' will be resolved to dictionary form which is supposed to be 'Ask'\n",
    "\n",
    "**Now let's do both of the process one by one**\n",
    "\n",
    "### Stemming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "#Porter STemmer is one way of Stripping out the Suffix\n",
    "# CONNECT\n",
    "# CONNECTIONS --> CONNECT\n",
    "# CONNECTING --> CONNECT\n",
    "# CONNECTION --> CONNECT\n",
    "# CONNECTED --> CONNECT\n",
    "\n",
    "def stemming_of_data(text_input):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [' '.join(stemmer.stem(word) for word in text.split()) for text in text_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_train = stemming_of_data(cleaned_train_data_from_stop_words)\n",
    "stemmed_test = stemming_of_data(cleaned_test_data_from_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(stemmed_train)\n",
    "X = cv.transform(stemmed_train)\n",
    "X_test = cv.transform(stemmed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.1 :0.88\n",
      "Accuracy for C=0.01 :0.87\n",
      "Accuracy for C=0.05 :0.88\n",
      "Accuracy for C=0.25 :0.88\n",
      "Accuracy for C=0.5 :0.87\n",
      "Accuracy for C=0.75 :0.87\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X,target,test_size=0.3,random_state = 4)\n",
    "\n",
    "\n",
    "for hyper_parameter in [0.1,0.01,0.05,0.25,0.5,0.75]:\n",
    "    lr_model_stemmed = LogisticRegression(C = hyper_parameter,max_iter = 300)\n",
    "    lr_model_stemmed.fit(X_train,y_train)\n",
    "    print('Accuracy for C=%s :%s'%(hyper_parameter,round(accuracy_score(y_val,lr_model_stemmed.predict(X_val)),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy:  0.8766\n"
     ]
    }
   ],
   "source": [
    "# Now let's check for the test set\n",
    "lr_model_stemmed = LogisticRegression(C = 0.05)\n",
    "lr_model_stemmed.fit(X,target)\n",
    "print('Final Accuracy: ',round(accuracy_score(target,lr_model_stemmed.predict(X_test)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using the STEMMING on the Data Frame which is free of **Stop Words** is giving us low accuracy, so we will try the whole lot same on the Normal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.1 :0.8809\n",
      "Accuracy for C=0.01 :0.8733\n",
      "Accuracy for C=0.05 :0.8829\n",
      "Accuracy for C=0.25 :0.878\n",
      "Accuracy for C=0.5 :0.8756\n",
      "Accuracy for C=0.75 :0.8743\n"
     ]
    }
   ],
   "source": [
    "stemmed_train = stemming_of_data(cleaned_train)\n",
    "stemmed_test = stemming_of_data(cleaned_test)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(stemmed_train)\n",
    "X = cv.transform(stemmed_train)\n",
    "X_test = cv.transform(stemmed_test)\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,target,test_size=0.3,random_state = 4)\n",
    "\n",
    "\n",
    "for hyper_parameter in [0.1,0.01,0.05,0.25,0.5,0.75]:\n",
    "    lr_model_stemmed = LogisticRegression(C = hyper_parameter,max_iter = 300)\n",
    "    lr_model_stemmed.fit(X_train,y_train)\n",
    "    print('Accuracy for C=%s :%s'%(hyper_parameter,round(accuracy_score(y_val,lr_model_stemmed.predict(X_val)),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy:  0.8771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Now let's check for the test set\n",
    "lr_model_stemmed = LogisticRegression(C = 0.05)\n",
    "lr_model_stemmed.fit(X,target)\n",
    "print('Final Accuracy: ',round(accuracy_score(target,lr_model_stemmed.predict(X_test)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatizing_of_data(text_input):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in text.split()]) for text in text_input]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Modelling on Text which is free of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.1: 0.8793333333333333\n",
      "Accuracy for C=0.01: 0.8734666666666666\n",
      "Accuracy for C=0.05: 0.88\n",
      "Accuracy for C=0.25: 0.8768\n",
      "Accuracy for C=0.5: 0.8726666666666667\n",
      "Accuracy for C=0.75: 0.8710666666666667\n"
     ]
    }
   ],
   "source": [
    "# For Stop Words\n",
    "lemmatized_reviews_train_stop_words_excluded = lemmatizing_of_data(cleaned_train_data_from_stop_words)\n",
    "lemmatized_reviews_test_stop_words_excluded = lemmatizing_of_data(cleaned_test_data_from_stop_words)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(lemmatized_reviews_train_stop_words_excluded)\n",
    "X = cv.transform(lemmatized_reviews_train_stop_words_excluded)\n",
    "X_test = cv.transform(lemmatized_reviews_test_stop_words_excluded)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, target, train_size = 0.7\n",
    ")\n",
    "\n",
    "for c in [0.1,0.01, 0.05, 0.25, 0.5, 0.75]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c,max_iter = 300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Modelling on Cleaned processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.1: 0.8805333333333333\n",
      "Accuracy for C=0.01: 0.8682666666666666\n",
      "Accuracy for C=0.05: 0.8812\n",
      "Accuracy for C=0.25: 0.8784\n",
      "Accuracy for C=0.5: 0.8762666666666666\n",
      "Accuracy for C=0.75: 0.8750666666666667\n"
     ]
    }
   ],
   "source": [
    "lemmatized_reviews_train = lemmatizing_of_data(cleaned_train)\n",
    "lemmatized_reviews_test = lemmatizing_of_data(cleaned_test)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(lemmatized_reviews_train)\n",
    "X = cv.transform(lemmatized_reviews_train)\n",
    "X_test = cv.transform(lemmatized_reviews_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, target, train_size = 0.7\n",
    ")\n",
    "\n",
    "for c in [0.1,0.01, 0.05, 0.25, 0.5, 0.75]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c,max_iter = 300)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "\n",
    "**What is N-Gram? and why we are going to use it?**\n",
    "N-Gram is a parameter of **Vectorization** which helps in identifying high power **bigrams(2 words)** as well.\n",
    "For example we want to use it to check what weight does words like **well worth** carry if joined. Does it increases the predictive power.\n",
    "\n",
    "Let's see it in practical application here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a Matrix which consists of Weights of each word, occuring number of times!\n",
    "ngram_cv = CountVectorizer(binary=True,ngram_range=(1,2))\n",
    "ngram_cv.fit(cleaned_train_data_from_stop_words)\n",
    "X = ngram_cv.transform(cleaned_train_data_from_stop_words)\n",
    "X_test = ngram_cv.transform(cleaned_test_data_from_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.1 :0.8839\n",
      "Accuracy for C=0.01 :0.8741\n",
      "Accuracy for C=0.05 :0.882\n",
      "Accuracy for C=0.25 :0.8837\n",
      "Accuracy for C=0.5 :0.8845\n",
      "Accuracy for C=0.75 :0.8836\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X,target,test_size=0.3)\n",
    "\n",
    "for c in [0.1,0.01,0.05,0.25,0.5,0.75]:\n",
    "    ngram_lr = LogisticRegression(C = c,max_iter = 300)\n",
    "    ngram_lr.fit(X_train,y_train)\n",
    "    print('Accuracy for C=%s :%s'%(c,round(accuracy_score(y_val,ngram_lr.predict(X_val)),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy:  0.8883\n"
     ]
    }
   ],
   "source": [
    "# Now let's check for the test set\n",
    "ngram_lr_test = LogisticRegression(C = 0.5)\n",
    "ngram_lr_test.fit(X,target)\n",
    "print('Final Accuracy: ',round(accuracy_score(target,ngram_lr_test.predict(X_test)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the **Train Acc is 88% and the Test Acc is also 88%** which is a decent result, but we did it on the **DataFrame which is cleaned from Stop Words**, now we will do it on our original Data frame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Positive Words are: \n",
      "('propagandait', 0.9872406751776253)\n",
      "('famousis', 0.42355665927160313)\n",
      "('pimple', 0.3654362259262154)\n",
      "('neuro', 0.36138242164183687)\n",
      "('walid', 0.3483109777878208)\n",
      "------***********------\n",
      "Top 5 Negative Words are: \n",
      "('structuring', -0.861275299952086)\n",
      "('negligee', -0.5464281663974457)\n",
      "('asexual', -0.4519909900147761)\n",
      "('hominid', -0.39828429048426606)\n",
      "('sm', -0.39645153425531376)\n"
     ]
    }
   ],
   "source": [
    "show_me_top_words(ngram_lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_cv_original = CountVectorizer(binary=True,ngram_range=(1,2))\n",
    "ngram_cv_original.fit(cleaned_train)\n",
    "X = ngram_cv_original.transform(cleaned_train)\n",
    "X_test = ngram_cv_original.transform(cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01 :0.8805\n",
      "Accuracy for C=0.05 :0.8867\n",
      "Accuracy for C=0.1 :0.8877\n",
      "Accuracy for C=0.25 :0.8891\n",
      "Accuracy for C=0.5 :0.8899\n",
      "Accuracy for C=0.75 :0.8888\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X,target,test_size = 0.3)\n",
    "\n",
    "for c in [0.01,0.05,0.1,0.25,0.5,0.75]:\n",
    "    ngram_original_lr = LogisticRegression(C = c,max_iter=300)\n",
    "    ngram_original_lr.fit(X_train,y_train)\n",
    "    print('Accuracy for C=%s :%s'%(c,round(accuracy_score(y_val,ngram_original_lr.predict(X_val)),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: %s 0.8987\n"
     ]
    }
   ],
   "source": [
    "# Test Performance for the same\n",
    "ngram_test_original = LogisticRegression(C = 0.5) # we are not using max_iter, just to show the warning which is recieved\n",
    "ngram_test_original.fit(X,target)\n",
    "print('Accuracy is:',round(accuracy_score(target,ngram_test_original.predict(X_test)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we get to see is that without using the **Stop Words cleaned DF**, we get an Accuracy of 90%(approx) which is quite amusing and our test accuracy is also approx 90%.\n",
    "There is an increase of almost 1.5% from **Lemmatization** and approx 2% from **Stemming**.\n",
    "Also, it is not needed that everytime, increasing the **ngrams** will give us better accuracy or performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Positive Words are: \n",
      "('minnieapolis', 0.8969108998668025)\n",
      "('thoroughfare', 0.35846085881026135)\n",
      "('saldana', 0.34478594426826975)\n",
      "('scenesanyway', 0.33108570217008254)\n",
      "('latham', 0.3292864357439761)\n",
      "------***********------\n",
      "Top 5 Negative Words are: \n",
      "('verbosity', -0.651051851761527)\n",
      "('alchemize', -0.4027733731446192)\n",
      "('interrogating', -0.3897407442638686)\n",
      "('bch', -0.3591348441867934)\n",
      "('talbot', -0.32489170326554556)\n"
     ]
    }
   ],
   "source": [
    "show_me_top_words(ngram_original_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SVM for N-Gram parameter and then we end this Exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01 :0.8888\n",
      "Accuracy for C=0.05 :0.8884\n",
      "Accuracy for C=0.1 :0.8885\n",
      "Accuracy for C=0.25 :0.8872\n",
      "Accuracy for C=0.5 :0.8869\n",
      "Accuracy for C=0.75 :0.8869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "for c in [0.01,0.05,0.1,0.25,0.5,0.75]:\n",
    "    svm_linear = LinearSVC(C=c)\n",
    "    svm_linear.fit(X_train,y_train)\n",
    "    print('Accuracy for C=%s :%s'%(c,round(accuracy_score(y_val,svm_linear.predict(X_val)),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.8941\n"
     ]
    }
   ],
   "source": [
    "# Test Set Check\n",
    "svm_test = LinearSVC(C = 0.25)\n",
    "svm_test.fit(X,target)\n",
    "print('Accuracy is:',round(accuracy_score(target,svm_test.predict(X_test)),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Positive Words are: \n",
      "('minnieapolis', 0.20478487267224568)\n",
      "('astronomical', 0.08464045307387719)\n",
      "('thoroughfare', 0.08130683705471237)\n",
      "('bathebo', 0.08061123497385746)\n",
      "('kidsmy', 0.07991351408947818)\n",
      "------***********------\n",
      "Top 5 Negative Words are: \n",
      "('verbosity', -0.14250695785955148)\n",
      "('talbot', -0.11069014475510253)\n",
      "('alchemize', -0.10210967112327878)\n",
      "('roman', -0.09983813841913194)\n",
      "('interrogating', -0.09888506164926293)\n"
     ]
    }
   ],
   "source": [
    "show_me_top_words(svm_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You for your Time!\n",
    "\n",
    "Do let me know any improvements!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
